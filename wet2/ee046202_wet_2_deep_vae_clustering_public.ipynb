{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Computer Assignment 2 - Variational Autoencoders & Clustering\n",
    "---\n",
    "### <a style='color:red'> Due Date: 26.01.2021 </a>\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Variational Autoencoders (VAEs)\n",
    "    * $\\beta$-VAE\n",
    "    * Conditional VAE (CVAE)\n",
    "    * BONUS: Generating Pokemons\n",
    "* Clustering\n",
    "    * Spectral Clustering for Image Segmentation\n",
    "    * Spectral Clustering vs. K-Means for Image Segmentation\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **BONUS**:\n",
    "    * 5 points - completing the Pokemon task with *fully-connected* layers (black & white version)\n",
    "    * 10 points - completing the Pokemon task with *convolutional* layers (RGB version)\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_wet2_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ee046202_wet2_id1_id2.zip` with content:\n",
    "        * `ee046202_wet2_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046202_wet2_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/info.png\" style=\"height:50px;display:inline\"> Tip\n",
    "---\n",
    "If you find it more convenient, you can copy the section to a new cell, and answer the question or rite the code just right below it. For example:\n",
    "\n",
    "#### Question 0\n",
    "1. What is the best course in the Technion?\n",
    "2. Why does no one pick Bulbasaur as first pokemon?\n",
    "3. Why is there no superhero named Catman?\n",
    "\n",
    "#### Answers - Q0\n",
    "\n",
    "#### Q0 - Section 1\n",
    "* Q: What is the best course in the Technion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"ANAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q0 - Section 2\n",
    "* Q: Why does no one pick Bulbasaur as first pokemon?\n",
    "\n",
    "It is really a riddle....\n",
    "\n",
    "#### Q0 - Section 3\n",
    "* Q: Why is there no superhero named Catman?\n",
    "\n",
    "I got nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/grand-master-key.png\" style=\"height:50px;display:inline\"> Part 1 - Variational Autoencoders - Prologue\n",
    "---\n",
    "In this section of the exercise we will analyze the VAE and introduce an enhacement called $\\beta$-VAE and also a variation of VAE that allows us some control over the latent space using conditional probability - Conditional Variational Autoencoder.\n",
    "\n",
    "For this part, you are provided the original implementation of the VAE from the tutorial. You will have to modify it throughout the tasks (yes yes, you can copy-paste from the original implementation).\n",
    "* **Note** - for better results you should tune the model!\n",
    "    * You can add layers / hidden units / different activations (ReLU, TanH, LeakyReLU, Sigmoid...)\n",
    "    * You can choose a different optimizer than Adam (SGD, RMSProp...), tune the learning rate...\n",
    "    * You can change the reconstruction loss (BCE, MSE, L1...)\n",
    "    * Other hyper-parameters like the batch-size, number of epochs and etc...\n",
    "\n",
    "We recommend running this part on Google Colab or on a GPU (if you have an access to one). Note that running on a GPU will lead to about x2 speedup in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 1\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original implementation from the tutorial - leave untouched (for your own sake), \n",
    "# copy-paste what you need to another cell\n",
    "\n",
    "# reparametrization trick\n",
    "def reparameterize(mu, logvar, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    This function applies the reparameterization trick:\n",
    "    z = mu(X) + sigma(X)^0.5 * epsilon, where epsilon ~ N(0,I)\n",
    "    :param mu: mean of x\n",
    "    :param logvar: log variance of x\n",
    "    :param device: device to perform calculations on\n",
    "    :return z: the sampled latent variable\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std).to(device)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "# encoder - Q(z|X)\n",
    "class VaeEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "       This class builds the encoder for the VAE\n",
    "       :param x_dim: input dimensions\n",
    "       :param hidden_size: hidden layer size\n",
    "       :param z_dim: latent dimensions\n",
    "       :param device: cpu or gpu\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim=28*28, hidden_size=256, z_dim=10, device=torch.device(\"cpu\")):\n",
    "        super(VaeEncoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.features = nn.Sequential(nn.Linear(x_dim, self.hidden_size),\n",
    "                                      nn.ReLU())\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.z_dim, bias=True)  # fully-connected to output mu\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.z_dim, bias=True)  # fully-connected to output logvar\n",
    "\n",
    "\n",
    "    def bottleneck(self, h):\n",
    "        \"\"\"\n",
    "        This function takes features from the encoder and outputs mu, log-var and a latent space vector z\n",
    "        :param h: features from the encoder\n",
    "        :return: z, mu, log-variance\n",
    "        \"\"\"\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        # use the reparametrization trick as torch.normal(mu, logvar.exp()) is not differentiable\n",
    "        z = reparameterize(mu, logvar, device=self.device)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        z, mu, logvar = VaeEncoder(X)\n",
    "        \"\"\"\n",
    "        h = self.features(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    \n",
    "class VaeDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "       This class builds the decoder for the VAE\n",
    "       :param x_dim: input dimensions\n",
    "       :param hidden_size: hidden layer size\n",
    "       :param z_dim: latent dimensions\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim=28*28, hidden_size=256, z_dim=10):\n",
    "        super(VaeDecoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(self.z_dim, self.hidden_size),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.hidden_size, self.x_dim),\n",
    "                                     nn.Sigmoid())\n",
    "        # why we use sigmoid? becaue the pixel values of images are in [0,1] and sigmoid(x) does just that!\n",
    "        # if you don't work with images, you don't have to use that.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        x_reconstruction = VaeDecoder(z)\n",
    "        \"\"\"\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Vae(torch.nn.Module):\n",
    "    def __init__(self, x_dim=28*28, z_dim=10, hidden_size=256, device=torch.device(\"cpu\")):\n",
    "        super(Vae, self).__init__()\n",
    "        self.device = device\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = VaeEncoder(x_dim, hidden_size, z_dim=z_dim, device=device)\n",
    "        self.decoder = VaeDecoder(x_dim, hidden_size, z_dim=z_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def sample(self, num_samples=1):\n",
    "        \"\"\"\n",
    "        This functions generates new data by sampling random variables and decoding them.\n",
    "        Vae.sample() actually generatess new data!\n",
    "        Sample z ~ N(0,1)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.z_dim).to(self.device)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        return x_recon, mu, logvar, z = Vae(X)\n",
    "        \"\"\"\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "    \n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, loss_type='bce'):\n",
    "    \"\"\"\n",
    "    This function calculates the loss of the VAE.\n",
    "    loss = reconstruction_loss - 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param recon_x: the reconstruction from the decoder\n",
    "    :param x: the original input\n",
    "    :param mu: the mean given X, from the encoder\n",
    "    :param logvar: the log-variance given X, from the encoder\n",
    "    :param loss_type: type of loss function - 'mse', 'l1', 'bce'\n",
    "    :return: VAE loss\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        recon_error = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    elif loss_type == 'l1':\n",
    "        recon_error = F.l1_loss(recon_x, x, reduction='sum')\n",
    "    elif loss_type == 'bce':\n",
    "        recon_error = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (recon_error + kl) / x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - $\\beta$-VAE\n",
    "---\n",
    "In the standard VAE an isotropic Gaussian ($p(z) \\sim \\mathcal{N}(0, I)$) is typically assumed as the prior distribution for z. Note that under this distribution the components of z are independent (e.g. disentangled - a disentangled representation can be defined as one where single latent units are sensitive to changes in single generative factors, while being relatively invariant to changes in other factors) which is exactly the property we would like our approximate posterior distribution (e.g. $q(z|x)$) to have. Thus, to encourage independence we increase the KL-divergence term in the ELBO by a factor of $\\beta$: $$ \\mathcal{L}_{\\beta-VAE} = -\\mathbb{E}_{q_{\\phi(z|x)}}[p_{\\theta}(x|z)] + \\beta \\cdot D_{KL}[q_{\\theta}(z|x) || p(z)]  $$\n",
    "   * Training is performed exactly the same as for the standard VAE.\n",
    "   * When $\\beta=1$, it is same as VAE.\n",
    "   * When $\\beta>1$, it applies a stronger constraint on the latent bottleneck and limits the representation capacity of $z$. \n",
    "       * For some conditionally independent generative factors, keeping them disentangled is the most efficient representation.\n",
    "   * When $0<\\beta<1$, it can be interpreted as optimizing an approximate log marginal likelihood bound under an alternative prior, regularized to prevent degeneracy (of the KL-divergence).\n",
    "   \n",
    "The tasks:\n",
    "1. Modify the loss function to support $\\beta$-VAE. The function should return the reconstruction loss, the kl-divergence (**without the multiplication by $\\beta$**) and and the total loss.\n",
    "    * The signature of the function should be: `beta_loss_function(recon_x, x, mu, logvar, loss_type='bce', beta=1)`\n",
    "    * The reconstruction loss, the kl-divergence and and the total loss should be normalzied by the batch size.\n",
    "    * The returned reconstruction loss and kl-divergence should be converted to numpy: `kl_d.data.cpu().numpy()` (but only them, not the total loss)\n",
    "2. Load the MNIST dataset, as in the tutorial, and create a train loader.\n",
    "3. For $\\beta=[0.05, 0.5, 1, 5]$, train a $\\beta$-VAE for 50 epochs and keep track of:\n",
    "    * The average reconstruction loss in each epoch\n",
    "    * The average KL-divergence in each epoch\n",
    "    * A checkpoint of the network in the format: `beta_(value of beta)_vae_50_epochs.pth` (there is an example in the tutorial)\n",
    "    * This may take a while, so go grab a coffee in the meantime :)\n",
    "4. For $\\beta=[0.05, 0.5, 1, 5]$, plot the KL-divergence and reconstruction error vs. epochs.\n",
    "5. For $\\beta=[0.05, 0.5, 1, 5]$, generate 5 samples from the VAE and plot them (`imshow`...). Run this a couple of times to get an impression of the samples for each $\\beta$.\n",
    "\n",
    "\n",
    "* Notes:\n",
    "    * Be organized - separate to different code cells if it keeps you organized.\n",
    "    * Make sure to properly define the hyper-parameters (see tutorial), and define the `device` automatically. Don't forget to send all the models and tensors to the device. We will run your code on a GPU.\n",
    "    * If you are not satisfied with the results, and you have time, you can try and increase the number of epochs to 100, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - $\\beta$-VAE\n",
    "---\n",
    "We will now analyze the results. Answer the following questions:\n",
    "1. Explain intuitively the loss function of $\\beta$-VAE. In your answer, explain the trade-off between the reconstruction loss and the KL-divergence and how it is affected by the $\\beta$ parameter. Hint: think about regularization as you learned in the ML course (for example, in linear regression).\n",
    "2. What is the main trend in the KL and reconstruction loss vs. epochs? In your answer, you should compare between the $\\beta$'s.\n",
    "3. For what values of $\\beta$ you would expect better reconstruction (why would we want better reconstruction?) and for what values you would expect higher-quality samples? In your answer, refer to the blurriness in the samples you plotted.\n",
    "4. Run the cell where you plot the samples (if you separated the cells for each $\\beta$, then run all of them) a couple of times (just hit Ctrl + Enter). For which value of $\\beta$ there is more *diversity* in the samples? (i.e., if out of 6 samples you get four 9's, it is not diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - Conditional VAE (CVAE)\n",
    "---\n",
    "Conditional Variational Autoencoder (CVAE) is an extension of Variational Autoencoder (VAE).\n",
    "However, as you may have noticed, we have no control on the data generation process on VAE. That is, for example, on MNIST, we could not control the latent space, and when we sampled, we would not know what digit would be generated. This could be problematic if we want to generate some specific data. As an example, suppose we want to convert a unicode character to handwriting. In vanilla VAE, there is no way to generate the handwriting based on the character that the user inputted. Concretely, suppose the user inputted character ‘2’, how do we generate handwriting image that is a character ‘2’? We couldn’t.\n",
    "\n",
    "Hence, CVAE was developed. Whereas VAE essentially models latent variables and data directly, CVAE models lantent variables and data, both conditioned to some random variables.\n",
    "Recall, on VAE, the objective is:\n",
    "$$\\log P(X) -  D_{KL}[Q(z|X) || P(z|X)]  =  \\mathbb{E}_{Q(z|X)}[\\log P(X|z)] -D_{KL}[Q(z|X)|| P(z)]  $$\n",
    "that is, we want to optimize the log likelihood of our data $P(X)$ under some “encoding” error. The original VAE model has two parts: the encoder $Q(x|Z)$ and the decoder $P(X|z)$.\n",
    "\n",
    "If we focus on the encoder, it models the latent variable $z$ directly based on $X$, and it doesn't care about the different types of $X$ (e.g., it doesn't care if it is 1 or an 8). But wait, this was all the idea of **unsupervised learning**, we have no access to labels. Well, in this case, we have some information (thus, CVAE is sometimes referred to as **semi-supervised** learning model).\n",
    "\n",
    "Similarly, in the decoder part, it only models $X$ directly based on the latent variable $z$. So, how do we tell the VAE what we want to generate? We can condition encoder and decoder to another things, let's denote them with $c$ (for \"condition\").\n",
    "\n",
    "* The encoder is now conditioned on 2 variables- $X, c$: $Q(z|X,c)$ and the decoder in now conditioned on- $z$, $c$: $P(X|z,c)$\n",
    "* Hence, our variational lower bound objective is now in this following form: $$ \\log P(X|c) -  D_{KL}[Q(z|X,c) || P(z|X,c)]  =  \\mathbb{E}_{Q(z|X,c)}[\\log P(X|z,c)] -D_{KL}[Q(z|X,c)|| P(z|c)] $$ (we just conditioned all of the distributions with a variable $c$)\n",
    "* So what is different? Almost nothing! We still model $P(z|c) \\sim \\mathcal{N}(0,I)$, and the rest are modeled by the neural network.\n",
    "* But how is it done in practice? Simple! **Concatenation**: instead of encoding $X$, we encoded $[X,c]$, that is, we concatenate them. Same for the decoder: we take the latent variable $z$ and concatenate with $c$ and then the input of the decoder is $[z,c]$.\n",
    "* In PyTorch, we concatenate with `x = torch.cat([x, x_cond], dim=1)` (the 0 dimension is the batch dimension).\n",
    "\n",
    "The tasks:\n",
    "* Load the Fashion-MNIST dataset, as in the tuorial, and create a train loader. Note that you get both the images and their **labels**.\n",
    "* The labels are the classes (0-9). In order to use them in the network we need to convert them to one-hot vectors (0 -> [1,0,0,0,0,0,0,0,0,0], 1 -> [0,1,0,0,0,0,0,0,0,0] ...). The length of the one hot vector in as the number of classes. You are given a function that converts ints to one-hot vectors, use it on the labels, before you perform the concatenation.\n",
    "* Modify the VAE architecture to support conditionals.\n",
    "    * Copy-paste the skeleton (the original VAE, from the begining of the tutorial), and just modify the current functions. Note that there **are very few** changes needed.\n",
    "    * Here are some tips, but feel free to implement as you wish, as long as it works:\n",
    "        * In Python, you can let a function input be `None`, and then if the user inputs something that is not `None`, the function would act different. Here is an example: `def encode(x, x_cond=None): if x_cond is not None: ...`\n",
    "        * Here are the parts that we recommend you change:\n",
    "            * In the Vae module:\n",
    "                * `def __init__(self, x_dim=28*28, z_dim=10, hidden_size=256, device=torch.device(\"cpu\"), cond_dim=None)`\n",
    "                * `def sample(self, num_samples=1, x_cond=None)`\n",
    "                * `def forward(self, x, x_cond=None)`\n",
    "            * Note that these are the minimal changes that can be done to implement VAE that supports CVAE. No need to modify VaeEncoder and VaeDecoder when we are using fully-connected layers. However, if we were to use convolutional layers, we would have to change also the encoder and decoder since convolutional layers work on images, and only after the images features from the convolutional layers have been extracted, we would concatenate the the condional $c$ (just before the fully-connected layers that output $\\mu, \\sigma$.\n",
    "* Train CVAE on the Fashion-MNIST dataset (100 epochs, at least). Use $\\beta$-VAE loss function (it shouldn't have changed from the regular VAE. Save a checkpoint of the network in the format: `fmnist_beta_(value of beta)_cvae_(number of epochs)_epochs.pth`. The rest of the hyper-parameters are up to you.\n",
    "* Plot $n_{samples}=6$ from the CVAE for 6 classes of your choosing.\n",
    "* **Tip**: this may take a while, so if everything seems to work, let it run on Google Colab and go grab another coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hots(batch, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts batch of integes numbers to one-hot vector given the vector length\n",
    "    :param batch: batch of values to convert\n",
    "    :param num_classes: length of the vector\n",
    "    :return: one_hot_batch\n",
    "    \"\"\"\n",
    "    one_hot_batch = torch.zeros(batch.size(0), num_classes).to(batch.device)\n",
    "    for i in range(batch.size(0)):\n",
    "        one_hot_batch[i, int(batch[i].data.cpu().item())] = 1\n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Conditional VAE\n",
    "---\n",
    "1. Can we perform interpolation of the latent space as we did in the tutorial? What is the meaning of doing **in-class** interpolation in the case of Fashion-MNIST? Explain.\n",
    "2. Why did we convert the the classes number to one-hot vectors? Think of the other inputs to the networks and the values that the neurons accept. What is the risk of using just one number as the condition instead of a vector?\n",
    "3. How is the quality of the samples? How can the quality be improved? In you answer, refer to the bluriness in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - BONUS - Pokemon: Gotta Generate 'Em All!\n",
    "---\n",
    "This is a **non-mandatory**, more challenging task. Come back here only if you are done with the rest of the exercise and want to take on a challenge.\n",
    "\n",
    "* Note - you should pick one of the following:\n",
    "    * Fully-connected - work with grayscale images (5 points)\n",
    "    * Convolutional - work with RGB images (10 points)\n",
    "* No matter what type of model you chose, you need to answer the bonus questions that follow the code.\n",
    "\n",
    "In this task we are going to (try) generate new pokemons! Our dataset includes ~900 pokemons. Each sample is a 60x60 image and the type of the pokemon (18 classes). The type is already in one-hot form. If you have access to a GPU, we recommend trying the convolutional version of this task.\n",
    "\n",
    "Let's look at the data, for the **fully-connected** version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon_dataset import PokemonDataset\n",
    "poke_data = PokemonDataset(root='./data/pokemon', rgb=False)\n",
    "sample_dataloader = DataLoader(poke_data, batch_size=6, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10 ,5))\n",
    "samples, labels = next(iter(sample_dataloader))\n",
    "for i in range(samples.size(0)):\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.imshow(samples[i][0].data.cpu().numpy(), cmap=\"gray\")\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "print(\"can you name these pokemons?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the types encoding\n",
    "poke_data.type_to_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**: Implement CVAE for the pokemon dataset, it should not be different than the one you implemented for Fashion-MNIST. The task is to define the model and tune the hyper-parameters. \n",
    "Note that due to being a really small dataset (only 900 examples!) you will need a really large number of epochs to get something. We don't expect to see actual pokemons, but we want to see the that the networks learned something. You will also need a larger latent space dimension, as pokemons are more complex than cloths.\n",
    "\n",
    "* If you have taken the **Computer Vision** course - you should add **data augementations** to the images, to create a more diverse dataset. Recommended augmentations: `RandomHorizontalFlip`, `ColorJitter`, random horizontal translation (up to 5 pixels).\n",
    "\n",
    "\n",
    "\n",
    "* Train CVAE on the pokemon dataset. Save a checkpoint once you are done: `pokemon_beta_(value of beta)_vae_(num epochs)_epochs.pth`.\n",
    "* Plot samples for at least 6 types of your choosing (try to find the better ones).\n",
    "* Plot reconstructions for at least 6 types of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have access to a GPU and feel adventurous (CNN version)?\n",
    "* If you have completed the fully-connected verion, you can just skip to the bonus questions.\n",
    "\n",
    "If you feel creative and want to work with CNNs, we are giving you the VaeCnnEncoder and VaeCnnDecoder architectures, and all you have to do is implement the Vae class using these, and run the VAE with RGB images.\n",
    "Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon_dataset import PokemonDataset\n",
    "poke_data = PokemonDataset(root='./data/pokemon', rgb=True)\n",
    "sample_dataloader = DataLoader(poke_data, batch_size=6, shuffle=True, drop_last=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10 ,5))\n",
    "samples, labels = next(iter(sample_dataloader))\n",
    "for i in range(samples.size(0)):\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.imshow(samples[i].permute(1, 2, 0).data.cpu().numpy())  # permute to (Height, Width, Channels)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "print(\"can you name these pokemons?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "* Impelement the convolutional CVAE and train it. Save a checkpoint `\"pokemon_cnn_beta_(value of beta)_vae_(num epochs)_epochs.pth\"`\n",
    "* Plot samples for at least 6 types of your choosing (try to find the better ones).\n",
    "* Plot reconstructions for at least 6 types of your choosing.\n",
    "\n",
    "\n",
    "* Note that the call to the loss function in the training loop is in the form: `loss = loss_function(x_recon, x.permute(0, 2, 3, 1), mu, logvar, loss_type='bce', beta=beta)`.\n",
    "    * This is because `x_recon` is in the shape (batch_size, H, W, C) and `x` is (batch_size, C, H, W).\n",
    "* Use a lower leraning rate (start with `1e-4`).\n",
    "\n",
    "* Components:\n",
    "    * Conv2D - `nn.Conv2d(in_channels, out_channels, kernel_size, stride)`\n",
    "    * Deconv2d - `nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)`\n",
    "    * Batch Normalization 1D - `nn.BatchNorm1d(num_features, affine=True)`\n",
    "    * Batch Normalization 2D - `nn.BatchNorm2d(num_features, affine=True)`\n",
    "    * Calculate the convolutional output size with `_get_conv_out(self, shape)` as in tutorial 8.\n",
    "    * FC/Linear - `nn.Linear(in, out)`\n",
    "\n",
    "#### Encoder Architecture - `VaeCnnEncoder(torch.nn.Module)`\n",
    "* Block 1:\n",
    "    * Conv2d - `in_channels=3, out_channels=128, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - 128 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 2:\n",
    "    * Conv2d - `in_channels=128, out_channels=64*4, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - $64*4$ features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 3:\n",
    "    * Conv2d - `in_channels=64*4, out_channels=64*8, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - $64*8$ features\n",
    "    * ReLU (or any other activation you want)\n",
    "* FC1 ($\\mu$) - `nn.Linear(self._get_conv_out(x_shape) + self.cond_dim, self.z_dim)`\n",
    "* FC1 ($\\Sigma$) - `nn.Linear(self._get_conv_out(x_shape) + self.cond_dim, self.z_dim)`\n",
    "\n",
    "#### Decoder Architecture - `VaeCnnDecoder(torch.nn.Module)`\n",
    "* FC1 ($z$) - `nn.Linear(self.z_dim (+cond_dim), 64 * 4 * 4 * 4)`\n",
    "* Batch Normalization 1D - $64*4*4*4$ features.\n",
    "* Block 1:\n",
    "    * Deconv2d - `in_channels=64 * 4, 64 * 2, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "    * Batch Normalization 2D - 128 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 2:\n",
    "    * Deconv2d - `in_channels=128, 64, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "    * Batch Normalization 2D - 64 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 3:\n",
    "    * Deconv2d - `in_channels=64, 64, kernel_size=(3, 3), stride=(2, 2), padding=2, output_padding=1`\n",
    "    * Batch Normalization 2D - 64 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Deconv2d - `in_channels=64, 3, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "* Sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Bonus Question\n",
    "---\n",
    "1. What do you think about the results on the Pokemon dataset? Name at least 2 reasons for the VAE somewhat low performance on the Pokemon dataest.\n",
    "2. Suggest ideas to improve the performance (at least 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/grand-master-key.png\" style=\"height:50px;display:inline\"> Part 2 - Spectral Clustering - Prologue\n",
    "---\n",
    "In this task we are going to explore Spectral Clustering for image segmentation. \n",
    "In computer vision, **image segmentation** is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n",
    "\n",
    "We are going tor compare K-Means to Spectral Clustering, and as you recall, K-Means is also a step in the the spectral clustering algorithm (remember where?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 2\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import warnings\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.cluster import spectral_clustering, KMeans\n",
    "\n",
    "warnings.filterwarnings(category=UserWarning, action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Spectral Clustering Demonstration\n",
    "---\n",
    "Run the next code cell and answer (below the code cell) the following questions:\n",
    "1. Run the cell 3 times (Ctrl + Enter). Why are the results different in each run?\n",
    "2. The `image_to_graph` function builds a graph from the image. Explain how the graph is built (you can read the documentation of this function with `help(image.img_to_graph)`. What are the weights of edges?\n",
    "3. Explain the code commands in lines (press Esc + L to view line numbers): 9, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run this cell\n",
    "start_time = time.time()\n",
    "original_img = Image.open(\"./data/a_logo_s.png\")\n",
    "grayscale_image = original_img.convert(\"L\")  # convert to grayscale\n",
    "img = np.array(grayscale_image)  # convert to np.array\n",
    "img[img==255] = 0  # zero-out the background, we don't care about it\n",
    "mask = img.astype(np.bool)  # create a mask for the graph-building function\n",
    "img = img.astype(float) / 255  # convert to numbers in [0,1]\n",
    "img += 1 / 255 + (0.2 / 255) * np.random.randn(*img.shape)  # add random noise\n",
    "\n",
    "graph = image.img_to_graph(img, mask=mask)  # build a graph with the gradients as weights\n",
    "graph.data = np.exp(-graph.data / graph.data.std())  # convert gradients to affinity\n",
    "\n",
    "labels = spectral_clustering(graph, n_clusters=60, eigen_solver='arpack')  # run spectral clustering\n",
    "label_im = np.full(mask.shape, -1.0)  # labels -> image\n",
    "label_im[mask] = labels  # assign correct labels\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax1 = fig.add_subplot(121)\n",
    "imsh = ax1.imshow(label_im, cmap=plt.cm.rainbow)\n",
    "ax1.set_axis_off()\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(imsh, cax=cax)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(np.array(original_img))\n",
    "ax2.set_axis_off()\n",
    "print(\"total time: {:.3f} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Image Segmentation with Spectral Clustering & K-Means\n",
    "---\n",
    "1. Run K-Means on `a_logo_s.png`. Tune the number of clusters and number of iterations and plot the results in the same way as in the demonstration.\n",
    "    * In order to use K-means on images, you need to reshape it: `img.reshape(-1,1)`\n",
    "    * To run K-Means: create an instance of K-Means: `k_means = KMeans(n_clusters=, max_iter=)` and *fit* the reshaped img: `k_means.fit(...)`\n",
    "    * To access the labels (=the assignment) of each pixel, call `labels = k_means.labels_`\n",
    "2. Load `iron_man_p_s.png` and run K-Means and Spectral Clustering. Tune each algorithm's hyper-parameters. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 4 - Image Segmentation with Spectral Clustering & K-Means\n",
    "---\n",
    "1. Compare the results of K-Means and Spectral Clustering on the Avengers logo. Which algoirthm yields more satisfying results, in your opinion? Explain.\n",
    "2. Compare the results of K-Means and Spectral Clustering on Iron Man. Explain how did you tune the parameters (i.e., how did you pick the number of clusters). What is the difference between the Avengers logo and Iron Man?\n",
    "3. Summarize the advantages and disadvantages of K-Means and Spectral Clustering for image segmentation (running time, type of images, amount of tuning...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
