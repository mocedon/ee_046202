{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Homework 3 - VAE, GAN & Clustering\n",
    "---\n",
    "### <a style='color:red'> Due Date: 26.01.2021 </a>\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Questions\n",
    "    * VAE Theory\n",
    "    * Fun with GANs\n",
    "    * Expectation Maximization\n",
    "\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
    "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, Overleaf, LyX...\n",
    "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
    "    * **Handwritten** - if we can't read your handwriting, we will automatically take off the points of the questions. Please write clearly. No bonus for handwritten submissions.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw3_id1_id2.ipynb`.\n",
    "    * If you answered the questions in a different file you should submit a `.pdf` file with the name `ee046202_hw3_id1_id2.pdf`.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - VAE Theory\n",
    "---\n",
    "As you recall, the objective function of the VAE is: $$ \\mathcal{L}_{VAE} = -\\mathbb{E}_{Q(z|X)}[\\log P(X|z)] + D_{KL}[Q(z|X)|| P(z)] $$ The first term is called the \"reconstruction error\" and the second is the KL-divergence. In this question we are going to derive mathematic properties of these terms.\n",
    "\n",
    "\n",
    "1.In the tutorial, you have seen how the reconstruction error for the Gaussian case is the MSE. For images, with pixel values in [0,1], this loss function is not that great, and it is better to use the **Binary Cross Entropy (BCE)** loss, which is defined as: $$ BCE = -[y \\log(p) +(1-y)\\log(1-p)] $$\n",
    "* $y$ - the true pixel value of the original image, $y \\in \\{0,1\\}$\n",
    "* $p$ - the output from the neural netwrok (estimated probabiliy that the pixel is 1), $p \\in [0,1]$\n",
    "   Assume that $P(X|z)$ is Benoulli-distributed, that is, $P(X|z) \\sim Bern(\\phi(x,z))$: $$ P(X|z) = \\begin{cases} \\phi(x,z) && \\text{ if } y=1 \\\\ 1 -\\phi(x,z) && \\text{ if } y=0 \\end{cases} $$\n",
    "    * $\\phi(x,z)$ is the output of the decoder for each pixel in the input image.\n",
    "    \n",
    "Show that $$-\\log P(x|z) = BCE.$$\n",
    "\n",
    "2.As you recall, we usually model the prior as $P(Z) \\sim \\mathcal{N}(0,I)$, but sometimes we want to model the prior as some other, perhaps more expressive distribution. We can even model it as a neural network, that is, $P(Z|c) \\sim \\mathcal{N}(\\mu_c, \\Sigma_c)$ where $\\mu_c, \\Sigma_c$ are the outputs of a neural network and $c$ is the input of the network. Derive the closed-form solution of the KL-divergence for this case. That is, derive: $$ D_{KL}[q_{\\phi}(z|X)|| p(z|c)] $$ You should express your answer with $\\mu_q, \\Sigma_q, \\mu_c, \\Sigma_C$ (no $z$'s). Recall that $q_{\\phi}(z|X) \\sim \\mathcal{N}(\\mu_q\\, \\Sigma_q)$. Use the same assumptions from the tutorial. Don't forget that the expectancy is over $z|x \\sim q(z|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Generative Adversarial Networks (GANs)\n",
    "---\n",
    "1. Recall that when the discriminator gets *too good*, generator gradient vanishes and learns nothing - **Vanisihng/Diminishing Gradient**. Consider the second term in the objective function which is relavent only for the generator: $$ \\mathbb{E}_{z\\sim q(z)} \\left[\\log(1 - D\\left(G(z)\\right)) \\right].$$ Show that if $D$ is confident (that the sample is fake - its output is 0), the gradient of the generator goes to 0.\n",
    "    * Hint: recall that the output of binary classification is the output of the *sigmoid* function, $\\sigma$.\n",
    "    \n",
    "2. In the proof of Nash equilibrium, we used the Jensen-Shannon Divergence (JSD): $$ JSD( p_G\\mid \\mid p_{data}) = JSD(p_{data} \\mid \\mid p_G) = \\frac{1}{2}KL\\left(p_{data} \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right) +  \\frac{1}{2}KL\\left(p_G \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right).$$ Denote $p_{data}=a, p_G=b$. Show that $$ 0 \\leq JSD(a\\mid \\mid b) \\leq 1.$$\n",
    "    * For the KL, use $\\log_2.$\n",
    "    \n",
    "3. Let's say you trained a GAN to generate images of apples (and assume the GAN converged and you can now generate nice images of apples). \n",
    "    * Can the **Discriminator** be used to classify between apples and non-apples? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Expectation Maximization\n",
    "---\n",
    "1. Prove Jensen's inequality for *concave* functions, which we used in the derivation of the EM algorithm. Show that for a concave function, $f(X)$, $$ \\mathbb{E}[f(X] \\leq f(\\mathbb{E}[X]) .$$\n",
    "2. Consider the EM algorithm as presented in the tutorial. Prove that if we replace $\\theta^{t+1}$ as defined in line 4, with $$ \\theta^{t+1} \\leftarrow \\overline{\\theta} ,$$ where $\\overline{\\theta}$ satisfies that, $$ \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right], $$ the modified EM algorithm is still assured to improve or to halt in each iteration.\n",
    "3. Show the following holds for any $\\{q_i\\}_{i=1}^n,$ $$ \\mathcal{F}(\\theta, \\{q_i(\\cdot)\\}_{i=1}^n) \\triangleq \\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(x_i, z)}{q_i(z)}\\right]$$ <br> $$ = \\sum_{i=1}^n-D_{KL}(q_i(z) || p(z|x_i, \\theta)) +\\log p(x_i|\\theta) $$\n",
    "4. Find $$\\{\\pi_l, \\mu_l, \\Sigma_l \\}_{l=1}^k = \\arg\\max_{\\theta}\\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i, z|\\theta) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
