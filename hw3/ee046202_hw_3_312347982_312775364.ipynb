{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "ee046202_hw_3_312347982_312775364.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlmGeWCuWBL9"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
        "---\n",
        "\n",
        "## Homework 3 - VAE, GAN & Clustering\n",
        "---\n",
        "### <a style='color:red'> Due Date: 26.01.2021 </a>\n",
        "\n",
        "\n",
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "* Questions\n",
        "    * VAE Theory\n",
        "    * Fun with GANs\n",
        "    * Expectation Maximization\n",
        "\n",
        "\n",
        "#### Use as many cells as you need\n",
        "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
        "\n",
        "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
        "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUewqmVWBME"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Gonen Weiss| gonen.weiss@campus.technion.ac.il| 312347982|\n",
        "|Alexander Balabanov| alexander.b@campus.technion.ac.il| 312775364|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJl1fBNkWBMF"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
        "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
        "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
        "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, Overleaf, LyX...\n",
        "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
        "    * **Handwritten** - if we can't read your handwriting, we will automatically take off the points of the questions. Please write clearly. No bonus for handwritten submissions.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw3_id1_id2.ipynb`.\n",
        "    * If you answered the questions in a different file you should submit a `.pdf` file with the name `ee046202_hw3_id1_id2.pdf`.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJvrF671WBMF"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I66HH-X5WBMG"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - VAE Theory\n",
        "---\n",
        "As you recall, the objective function of the VAE is: $$ \\mathcal{L}_{VAE} = -\\mathbb{E}_{Q(z|X)}[\\log P(X|z)] + D_{KL}[Q(z|X)|| P(z)] $$ The first term is called the \"reconstruction error\" and the second is the KL-divergence. In this question we are going to derive mathematic properties of these terms.\n",
        "\n",
        "\n",
        "1.In the tutorial, you have seen how the reconstruction error for the Gaussian case is the MSE. For images, with pixel values in [0,1], this loss function is not that great, and it is better to use the **Binary Cross Entropy (BCE)** loss, which is defined as: $$ BCE = -[y \\log(p) +(1-y)\\log(1-p)] $$\n",
        "* $y$ - the true pixel value of the original image, $y \\in \\{0,1\\}$\n",
        "* $p$ - the output from the neural netwrok (estimated probabiliy that the pixel is 1), $p \\in [0,1]$\n",
        "   Assume that $P(X|z)$ is Benoulli-distributed, that is, $P(X|z) \\sim Bern(\\phi(x,z))$: $$ P(X|z) = \\begin{cases} \\phi(x,z) && \\text{ if } y=1 \\\\ 1 -\\phi(x,z) && \\text{ if } y=0 \\end{cases} $$\n",
        "    * $\\phi(x,z)$ is the output of the decoder for each pixel in the input image.\n",
        "    \n",
        "Show that $$-\\log P(x|z) = BCE.$$\n",
        "\n",
        "2.As you recall, we usually model the prior as $P(Z) \\sim \\mathcal{N}(0,I)$, but sometimes we want to model the prior as some other, perhaps more expressive distribution. We can even model it as a neural network, that is, $P(Z|c) \\sim \\mathcal{N}(\\mu_c, \\Sigma_c)$ where $\\mu_c, \\Sigma_c$ are the outputs of a neural network and $c$ is the input of the network. Derive the closed-form solution of the KL-divergence for this case. That is, derive: $$ D_{KL}[q_{\\phi}(z|X)|| p(z|c)] $$ You should express your answer with $\\mu_q, \\Sigma_q, \\mu_c, \\Sigma_C$ (no $z$'s). Recall that $q_{\\phi}(z|X) \\sim \\mathcal{N}(\\mu_q\\, \\Sigma_q)$. Use the same assumptions from the tutorial. Don't forget that the expectancy is over $z|x \\sim q(z|x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZJy51SkXL2o"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:50px;display:inline\"> Solution 1 - VAE Theory\r\n",
        "---\r\n",
        "1. \r\n",
        "The whole image $x$ is variationally encoded into $z$, and $z$ is decoded into $\\hat{x}$.<br>\r\n",
        "Say we have a pixel whose true value was $y$, and the reconstructed value was $p=\\phi(x,z)$ (at that same pixel!).<br>\r\n",
        "We assume that $P(X|z) \\sim Bern(\\phi(x,z))$:\r\n",
        "$$ P(X|z) = \\begin{cases} \\phi(x,z) && \\text{ if } y=1 \\\\ 1 -\\phi(x,z) && \\text{ if } y=0 \\end{cases} $$\r\n",
        "We shall compute BCE in either case:\r\n",
        "* If $y=0$:\r\n",
        "\\begin{align*}\r\n",
        "BCE|_{y=0} &= -[0\\log(p) + (1-0)\\log(1-p)|_{y=0} \\\\\r\n",
        "&= -\\log(1-\\phi(x,z))|_{y=0} \\\\\r\n",
        "&= -\\log P(X|z)|_{y=0}\r\n",
        "\\end{align*}\r\n",
        "* If $y=1$:\r\n",
        "\\begin{align*}\r\n",
        "BCE|_{y=1} &= -[1\\log(p) + (1-1)\\log(1-p)|_{y=1} \\\\\r\n",
        "&= -\\log(\\phi(x,z))|_{y=1} \\\\\r\n",
        "&= -\\log P(X|z)|_{y=1}\r\n",
        "\\end{align*}\r\n",
        "* $\\Longrightarrow$ From the two cases we derive:\r\n",
        "$$ BCE \\equiv -\\log P(X|z) $$\r\n",
        "<br>\r\n",
        "2. \r\n",
        "Recall that a multivariate normal distributing random vector $ G \\sim \\mathcal{N}(\\mu,\\Sigma) $ has a probability density function (PDF) of:\r\n",
        "$$ f_G(g) = (2\\pi)^{-\\frac{d}{2}} \\det(\\Sigma)^{-\\frac{1}{2}} \\cdot e^{-\\frac{1}{2} (g-\\mu)^T\\Sigma^{-1}(g-\\mu)} $$\r\n",
        "As a helping formula, note that:\r\n",
        "\\begin{align*}\r\n",
        "\\mathbb{E} \\Big[ (G-\\mu)^T\\Sigma^{-1}(G-\\mu) \\Big]\r\n",
        "&= \\mathbb{E} \\Big[ G^T\\Sigma^{-1}G - G^T\\Sigma^{-1}\\mu - \\mu^T\\Sigma^{-1}G + \\mu^T\\Sigma^{-1}\\mu \\Big] \\\\\r\n",
        "&= \\mathbb{E} \\Big[ G^T\\Sigma^{-1}G - 2\\mu^T\\Sigma^{-1}G + \\mu^T\\Sigma^{-1}\\mu \\Big] \\\\\r\n",
        "&= \\mathbb{E} \\Big[ G^T\\Sigma^{-1}G \\Big] - 2\\mu^T\\Sigma^{-1}\\mathbb{E}[G] + \\mu^T\\Sigma^{-1}\\mu \\\\\r\n",
        "\\end{align*}\r\n",
        "And in the case of a diagonal covariance matrix:\r\n",
        "\\begin{align*}\r\n",
        "\\mathbb{E} \\Big[ G^T\\Sigma^{-1}G \\Big]\r\n",
        "&= \\mathbb{E} \\Big[ \\sum_{i} \\frac{G_i^2}{\\Sigma_{ii}^2} \\Big] \\\\\r\n",
        "&= \\sum_{i} \\frac{\\mathbb{E}[G_i^2]}{\\Sigma_{ii}^2} \\\\\r\n",
        "&= \\sum_{i} \\frac{Var[G_i] + \\mathbb{E}[G_i]^2}{\\Sigma_{ii}^2} \\\\\r\n",
        "\\end{align*}\r\n",
        "We avoided on purpose the using $\\mu$ and $\\Sigma$ because the $\\mathbb{E}[\\cdot]$ is not necessarily with respect to $G \\sim f_G(g)$ !<br>\r\n",
        "In our case, we are having:\r\n",
        "* $ Z|c \\sim \\mathcal{N}(\\mu_c, \\Sigma_c) = p(Z|c)$\r\n",
        "* $ Z|x \\sim \\mathcal{N}(\\mu_q, \\Sigma_q) = q_{\\phi}(Z|x) $\r\n",
        "* We assume that both $\\Sigma_c$ and $\\Sigma_q$ are diagonal (independent features)!<br>This allows using the last formula and calculating the determinant:\r\n",
        "$$ \\log(\\det(\\Sigma)) = \\log \\prod_i \\Sigma_{ii}^2 = \\sum_i \\log \\Sigma_{ii}^2 $$\r\n",
        "\\begin{align*}\r\n",
        "D_{KL}\\Big[q_{\\phi}(Z|x) || p(Z|c)\\Big]\r\n",
        "&\\triangleq \\mathbb{E}_{q_{\\phi}(Z|x)}\\left[ \\log \\frac{q_{\\phi}(Z|x)}{p(Z|c)} \\right] \\\\\r\n",
        "&= \\mathbb{E}_{q_{\\phi}(Z|x)}\\left[ \\log \\frac{(2\\pi)^{-\\frac{d}{2}} \\det(\\Sigma_q)^{-\\frac{1}{2}} \\cdot e^{-\\frac{1}{2} (Z-\\mu_q)^T\\Sigma_q^{-1}(Z-\\mu_q)}}{(2\\pi)^{-\\frac{d}{2}} \\det(\\Sigma_c)^{-\\frac{1}{2}} \\cdot e^{-\\frac{1}{2} (Z-\\mu_c)^T\\Sigma_c^{-1}(Z-\\mu_c)}} \\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\log \\frac{\\det(\\Sigma_q)}{\\det(\\Sigma_c)} - \\frac{1}{2}\\mathbb{E}_{q_{\\phi}(Z|x)} \\left[ (Z-\\mu_q)^T\\Sigma_q^{-1}(Z-\\mu_q) - (Z-\\mu_c)^T\\Sigma_c^{-1}(Z-\\mu_c) \\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\left[ \\log \\frac{\\det(\\Sigma_q)}{\\det(\\Sigma_c)}\r\n",
        "+ \\sum_{i} \\frac{Var_{q_{\\phi}(Z|x)}[Z_i] + \\mathbb{E}_{q_{\\phi}(Z|x)}[Z_i]^2}{\\Sigma_{q_{ii}}^2} - 2\\mu_q^T\\Sigma_q^{-1}\\mathbb{E}_{q_{\\phi}(Z|x)}[Z] + \\mu_q^T\\Sigma_q^{-1}\\mu_q\r\n",
        "- \\sum_{i} \\frac{Var_{q_{\\phi}(Z|x)}[Z_i] + \\mathbb{E}_{q_{\\phi}(Z|x)}[Z_i]^2}{\\Sigma_{c_{ii}}^2} + 2\\mu_c^T\\Sigma_c^{-1}\\mathbb{E}_{q_{\\phi}(Z|x)}[Z] - \\mu_c^T\\Sigma_c^{-1}\\mu_c\r\n",
        "\\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\left[ \\log(\\det(\\Sigma_q)) - \\log(\\det(\\Sigma_c))\r\n",
        "+ \\sum_{i} \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2} - 2\\mu_q^T\\Sigma_q^{-1}\\mu_q + \\mu_q^T\\Sigma_q^{-1}\\mu_q\r\n",
        "- \\sum_{i} \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{c_{ii}}^2} + 2\\mu_c^T\\Sigma_c^{-1}\\mu_q - \\mu_c^T\\Sigma_c^{-1}\\mu_c\r\n",
        "\\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\left[ \\sum_{i} \\log(\\Sigma_{q_{ii}}^2) - \\sum_{i} \\log(\\Sigma_{c_{ii}}^2)\r\n",
        "+ \\sum_{i} \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "- 2 \\sum_{i} \\frac{\\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "+ \\sum_{i} \\frac{\\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "- \\sum_{i} \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "+ 2 \\sum_{i} \\frac{\\mu_{q_i}\\mu_{c_i}}{\\Sigma_{c_{ii}}^2}\r\n",
        "- \\sum_{i} \\frac{\\mu_{c_i}^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "\\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\sum_{i} \\left[ \\log(\\Sigma_{q_{ii}}^2) - \\log(\\Sigma_{c_{ii}}^2)\r\n",
        "+ \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "- 2 \\frac{\\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "+ \\frac{\\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "- \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "+ 2 \\frac{\\mu_{q_i}\\mu_{c_i}}{\\Sigma_{c_{ii}}^2}\r\n",
        "- \\frac{\\mu_{c_i}^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "\\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\sum_{i} \\left[ \\log\\left(\\frac{\\Sigma_{q_{ii}}^2}{\\Sigma_{c_{ii}}^2}\\right)\r\n",
        "+ \\frac{\\Sigma_{q_{ii}}^2 + 0\\cdot\\mu_{q_i}^2}{\\Sigma_{q_{ii}}^2}\r\n",
        "- \\frac{\\Sigma_{q_{ii}}^2 + \\mu_{q_i}^2 - 2\\mu_{q_i}\\mu_{c_i} + \\mu_{c_i}^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "\\right] \\\\\r\n",
        "&= -\\frac{1}{2} \\sum_{i} \\left[ \\log\\left(\\frac{\\Sigma_{q_{ii}}^2}{\\Sigma_{c_{ii}}^2}\\right)\r\n",
        "+ 1 - \\frac{\\Sigma_{q_{ii}}^2}{\\Sigma_{c_{ii}}^2} - \\frac{(\\mu_{q_i} - \\mu_{c_i})^2}{\\Sigma_{c_{ii}}^2}\r\n",
        "\\right] \\\\\r\n",
        "&= \\frac{1}{2} \\sum_{i=1}^d \\left[ \\frac{\\Sigma_{q_{ii}}^2}{\\Sigma_{c_{ii}}^2} + \\frac{(\\mu_{q_i} - \\mu_{c_i})^2}{\\Sigma_{c_{ii}}^2} - 1 - \\log\\left(\\frac{\\Sigma_{q_{ii}}^2}{\\Sigma_{c_{ii}}^2}\\right) \\right] \\\\\r\n",
        "\\end{align*}\r\n",
        "Note: We always kept in mind that $ \\mu_q = \\mu_q(x), \\Sigma_q = \\Sigma_q(x) $ are function of the input image $x$, and avoided writing it for simplicity.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw-50Di40Onj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Generative Adversarial Networks (GANs)\r\n",
        "---\r\n",
        "1. \r\n",
        "The gradient that used to update the weights of the Generator $\\theta_g$ is\r\n",
        "$$ \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\log(1 - D(G(z^{(i)}))) $$\r\n",
        "For the sake of the question it is sufficient to solve for $m=1$.<br>\r\n",
        "We shall prove the Vanisihng Gradient effect in the case where $ \\forall z: D(G(z)) \\approx 0 $.<br>\r\n",
        "Since the discriminator is a classifier, it is very reasonable to assume that after a certain mapping $ y := f(G(z)) $ the descrimnator's call is $ D(G(z)) = \\sigma(f(G(z))) = \\sigma(y) $, for the sigmoid-logistic-sigmoid function $ \\sigma(y) := \\frac{1}{1+e^{-y}} $ whose derivative is $ \\frac{d\\sigma(y)}{dy} = \\sigma(y) \\cdot (1-\\sigma(y)) $.<br>\r\n",
        "Using the chain rule:\r\n",
        "\\begin{align*}\r\n",
        "\\frac{d\\big\\{\\log(1 - D(G(z)))\\big\\}}{d{\\theta_{g_i}}}\r\n",
        "&= \\frac{1}{1 - D(G(z))} \\cdot \\frac{d\\big\\{1 - D(G(z))\\big\\}}{d{\\theta_{g_i}}} \\\\\r\n",
        "&= -\\frac{1}{1 - D(G(z))} \\cdot \\frac{d\\big\\{D(G(z))\\big\\}}{d{\\theta_{g_i}}} \\\\\r\n",
        "&= -\\frac{1}{1 - D(G(z))} \\cdot \\frac{d\\big\\{\\sigma(y)\\big\\}}{d{\\theta_{g_i}}} \\\r\n",
        "\\\\\r\n",
        "&= -\\frac{1}{1 - D(G(z))} \\cdot \\sigma(y) \\cdot (1-\\sigma(y)) \\cdot \\frac{dy}{d{\\theta_{g_i}}} \\\\\r\n",
        "&= -\\frac{1}{1 - D(G(z))} \\cdot D(G(z)) \\cdot (1-D(G(z))) \\cdot \\frac{d\\big\\{D(G(z))\\big\\}}{d{\\theta_{g_i}}} \\\\\r\n",
        "&\\stackrel{\\small{D(G(z)) \\approx 0}}{\\approx} -\\frac{1}{1 - 0} \\cdot 0 \\cdot (1-0) \\cdot \\frac{d\\big\\{D(G(z))\\big\\}}{d{\\theta_{g_i}}} \\\\\r\n",
        "&= 0\r\n",
        "\\end{align*}\r\n",
        "Thus the gradient will be too small and the training will take too long to converge.\r\n",
        "    \r\n",
        "2. In the proof of Nash equilibrium, we used the Jensen-Shannon Divergence (JSD): $$ JSD( p_G\\mid \\mid p_{data}) = JSD(p_{data} \\mid \\mid p_G) = \\frac{1}{2}KL\\left(p_{data} \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right) +  \\frac{1}{2}KL\\left(p_G \\mid \\mid \\left(\\frac{p_{data} + p_G}{2}\\right)\\right).$$ Denote $p_{data}=a, p_G=b$. Show that $$ 0 \\leq JSD(a\\mid \\mid b) \\leq 1.$$\r\n",
        "    * For the KL, use $\\log_2.$\r\n",
        "    \r\n",
        "3. \r\n",
        "After the training process of the GAN has finished succesfully, Nash Equilibrium must have been achieved, theoretically speaking, and the Discriminator has got to play with the same response to every Generator's output! (Like in the Game theory theorem we saw in class)<br>\r\n",
        "Furthermore, the Discriminator would provide the same response for every possible input image, independent of its content!<br>\r\n",
        "Hence, the discriminator can no longer be used to classify  between apples and non-apples, and in fact shouldn't perform any better than a random \"fair coin\" classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnC3VQwwWBMI"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:50px;display:inline\"> Solution 2 - Generative Adversarial Networks (GANs)\n",
        "---\n",
        "1. Recall that when the discriminator gets *too good*, generator gradient vanishes and learns nothing - **Vanisihng/Diminishing Gradient**. Consider the second term in the objective function which is relavent only for the generator: $$ \\mathbb{E}_{z\\sim q(z)} \\left[\\log(1 - D\\left(G(z)\\right)) \\right].$$ Show that if $D$ is confident (that the sample is fake - its output is 0), the gradient of the generator goes to 0.\n",
        "    * Hint: recall that the output of binary classification is the output of the *sigmoid* function, $\\sigma$.\n",
        "    \n",
        "2. \n",
        "Denote $ c:= \\frac{a+b}{2} $.<br>\n",
        "The first inequality is straight forward. Recall that $ KL(\\cdot||\\cdot) \\geq 0 $:\n",
        "$$ JSD(a||b) \\triangleq \\frac{1}{2}KL(a||c) + \\frac{1}{2}KL(b||c) \\geq 0 + 0 = 0 $$\n",
        "For the sceond inequality, first shall prove that $ KL(a||c) \\leq 1 $ (same goes for $b$):\n",
        "\\begin{align*}\n",
        "KL(a||c) \\triangleq \\mathbb{E}_{x \\sim a}[\\lg(\\frac{a}{c})]\n",
        "&= \\int_{x} a \\lg(\\frac{a}{c})dx \\\\\n",
        "&= \\int_{x} a \\lg(\\frac{2a}{a+b})dx \\\\\n",
        "&= \\int_{x} a \\big[ \\lg(2) + \\lg(\\frac{a}{a+b}) \\big]dx \\\\\n",
        "&= \\int_{x} a dx + \\int_{x} a \\lg(\\frac{a}{a+b}) dx \\\\\n",
        "&= 1 + \\int_{x} a \\lg(\\frac{a}{a+b}) dx \\\\\n",
        "&\\leq 1 + \\int_{x} a \\lg(1) dx = 1 + 0 = 1 \\\\\n",
        "\\end{align*}\n",
        "Where the inequality step was due to the monotonicity of $\\log(\\cdot)$:\n",
        "$$ \\forall x: a(x),b(x) \\geq 0 \\Rightarrow \\frac{a(x)}{a(x)+b(x)} \\leq 1 \\Rightarrow \\lg\\left(\\frac{a(x)}{a(x)+b(x)}\\right) \\leq \\lg(1) = 0 $$\n",
        "Using the above upper bound we now get:\n",
        "$$ JSD(a||b) \\triangleq \\frac{1}{2}KL(a||c) + \\frac{1}{2}KL(b||c) \\leq \\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot 1 = \\frac{1}{2} + \\frac{1}{2} = 1 $$\n",
        "And to conclude we have shown:\n",
        "$$ 0 \\leq JSD(a||b) \\leq 1 $$\n",
        "    \n",
        "3. Let's say you trained a GAN to generate images of apples (and assume the GAN converged and you can now generate nice images of apples). \n",
        "    * Can the **Discriminator** be used to classify between apples and non-apples? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CCPZknGWBML"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Expectation Maximization\n",
        "---\n",
        "1. Prove Jensen's inequality for *concave* functions, which we used in the derivation of the EM algorithm. Show that for a concave function, $f(X)$, $$ \\mathbb{E}[f(X] \\leq f(\\mathbb{E}[X]) .$$\n",
        "2. Consider the EM algorithm as presented in the tutorial. Prove that if we replace $\\theta^{t+1}$ as defined in line 4, with $$ \\theta^{t+1} \\leftarrow \\overline{\\theta} ,$$ where $\\overline{\\theta}$ satisfies that, $$ \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right], $$ the modified EM algorithm is still assured to improve or to halt in each iteration.\n",
        "3. Show the following holds for any $\\{q_i\\}_{i=1}^n,$ $$ \\mathcal{F}(\\theta, \\{q_i(\\cdot)\\}_{i=1}^n) \\triangleq \\sum_{i=1}^n\\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log \\frac{p(x_i, z)}{q_i(z)}\\right]$$ <br> $$ = \\sum_{i=1}^n-D_{KL}(q_i(z) || p(z|x_i, \\theta)) +\\log p(x_i|\\theta) $$\n",
        "4. Find $$\\{\\pi_l, \\mu_l, \\Sigma_l \\}_{l=1}^k = \\arg\\max_{\\theta}\\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i, z|\\theta) \\right] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuFyRvmIiukR"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:50px;display:inline\"> Solution 3 - Expectation Maximization\r\n",
        "---\r\n",
        "1. \r\n",
        "A concave function sustains:\r\n",
        "$$f(x_2) - f(x_1) \\geq f'(x_1)(x_2 - x_1)$$\r\n",
        "  Shall we take $x_1 = \\mu = \\mathbb{E}[X]$ and $ x_2 = X$\r\n",
        "$$f(X) - f(\\mu) \\geq f'(\\mu)(X - \\mu)$$\r\n",
        "  Enequality holds for mean, therefore\r\n",
        "$$\\mathbb{E}[f(X) - f(\\mu)] \\geq \\mathbb{E}[f'(\\mu)(X - \\mu)]$$\r\n",
        "$$\\mathbb{E}[f(X)] - \\mathbb{E}[f(\\mu)] \\geq f'(\\mu) \\mathbb{E}[X] - \\mu$$\r\n",
        "$$\\mathbb{E}[f(X)] - f(\\mu) \\geq f'(\\mu)(\\mu - \\mu) = 0$$\r\n",
        "$$\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])$$\r\n",
        "\r\n",
        "2. \r\n",
        "What we are actually showing here is that we do not have to strictly choose the $\\theta$ that maximizes $Q(\\Theta; \\Theta^t)$ in order to improve constantly. In fact, taking any $\\theta$ for which $Q(\\Theta; \\Theta^t)$ increases, the entire $\\mathcal{L}(X|\\Theta)$ increases (and improves!).<br>\r\n",
        "Formally, we want to show:\r\n",
        "$$ \\forall \\overline{\\theta} \\text{ s.t.} \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\overline{\\theta}) \\right] \\geq \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i,z|\\theta^t) \\right]: $$\r\n",
        "$$ \\theta^{t+1} := \\overline{\\theta} \\Longrightarrow \\mathcal{L}(X|\\theta^{t+1}) = \\mathcal{L}(X|\\overline{\\theta}) \\geq \\mathcal{L}(X|\\theta^t) $$<br>\r\n",
        "* For $q_i^*(z) := p(z|x_i, \\theta^t)$ (and for every other $q$):\r\n",
        "$$ \\mathcal{L}(X|\\overline{\\theta}) \\geq \\mathcal{F}(\\overline{\\theta}, \\{q_i^*(\\cdot)\\}_{i=1}^n) $$<br>\r\n",
        "* Thanks to the update of $\\theta$ (the M-Step and the property of $\\overline{\\theta}$):\r\n",
        "$$ \\mathcal{F}(\\overline{\\theta}, \\{q_i^*(\\cdot)\\}_{i=1}^n) = \\sum_{i=1}^n\\big[ \\mathbb{E}_{z \\sim q_i^*}\\log p(x_i, z | \\overline{\\theta}) +\\mathcal{H}(q_i^*(z)) \\big] \\geq \\sum_{i=1}^n\\big[ \\mathbb{E}_{z \\sim q_i}\\log p(x_i, z | \\theta^t) +\\mathcal{H}(q_i^*(z)) \\big] = \\mathcal{F}(\\theta^t, \\{q_i^*(\\cdot)\\}_{i=1}^n) $$<br>\r\n",
        "* As we saw in class, for this specific $q$:\r\n",
        "$$ \\mathcal{F}(\\theta^t, \\{q_i^*(\\cdot)\\}_{i=1}^n) = \\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^*}\\log\\big[\\frac{p(x_i, z|\\theta^t)}{q_i^*(z)}\\big] = \\sum_{i=1}^n \\mathbb{E}_{z \\sim p(z|x_i, \\theta^t)}\\log\\big[\\frac{p(x_i, z|\\theta^t)}{p(z|x_i, \\theta^t)}\\big] = \\sum_{i=1}^n \\mathbb{E}_{z \\sim p(z|x_i, \\theta^t)}\\log(p(x_i|\\theta^t)) = \\sum_{i=1}^n \\log p(x_i|\\theta^t) = \\mathcal{L}(X|\\theta^t)$$\r\n",
        "* $ \\Longrightarrow$ Thus we got an inequality chain of what we wanted:\r\n",
        "$$  \\mathcal{L}(X|\\overline{\\theta}) \\geq \\mathcal{F}(\\overline{\\theta}, \\{q_i^*(\\cdot)\\}_{i=1}^n) \\geq \\mathcal{F}(\\theta^t, \\{q_i^*(\\cdot)\\}_{i=1}^n) = \\mathcal{L}(X|\\theta^t)$$\r\n",
        "\r\n",
        "3. \r\n",
        "Let there be a $\\{q_i\\}_{i=1}^n$.\r\n",
        "$$\r\n",
        "\\begin{align}\r\n",
        "\\mathcal{F}(\\theta ,\\{{{q}_{i}}(\\cdot )\\}_{i=1}^{n}) &\\triangleq \\sum\\limits_{i=1}^{n}{{{\\mathbb{E}}_{z\\sim q_{i}^{*}(\\cdot )}}}\\left[ \\log \\frac{p({{x}_{i}},z|\\theta )}{{{q}_{i}}(z)} \\right] \\\\ \r\n",
        " & =\\sum\\limits_{i=1}^{n}{{{\\mathbb{E}}_{z\\sim q_{i}^{*}(\\cdot )}}\\left[ \\log \\frac{p(z|{{x}_{i}},\\theta )p({{x}_{i}}|\\theta )}{{{q}_{i}}(z)} \\right]} \\\\ \r\n",
        " & =\\sum\\limits_{i=1}^{n}{{{\\mathbb{E}}_{z\\sim q_{i}^{*}(\\cdot )}}\\left[ -\\log \\frac{{{q}_{i}}(z)}{p(z|{{x}_{i}},\\theta )}+\\log p({{x}_{i}}|\\theta ) \\right]} \\\\ \r\n",
        " & =\\sum\\limits_{i=1}^{n}{\\left\\{ {{\\mathbb{E}}_{z\\sim q_{i}^{*}(\\cdot )}}\\left[ -\\log \\frac{{{q}_{i}}(z)}{p(z|{{x}_{i}},\\theta )} \\right]+{{\\mathbb{E}}_{z\\sim q_{i}^{*}(\\cdot )}}\\left[ \\log p({{x}_{i}}|\\theta ) \\right] \\right\\}} \\\\ \r\n",
        " & =\\sum\\limits_{i=1}^{n}{\\left\\{ -{{D}_{KL}}({{q}_{i}}(z)||p(z|{{x}_{i}},\\theta ))+\\log p({{x}_{i}}|\\theta ) \\right\\}} \\\\ \r\n",
        "\\end{align}\r\n",
        "$$\r\n",
        "With the second equality being the definition of conditonal probability, the third is arithmatics of logarithms, the fourth is linearity of $\\mathbb{E}[\\cdot]$ and the fifth is the definition of $D_{KL}(\\cdot||\\cdot)$ and the expectance of a deterministic in $Z$.\r\n",
        "\r\n",
        "4. \r\n",
        "This is the M-Step. We need to find the parameters $\\theta$ that maximize the conditional expected value:\r\n",
        "$$ {\\{\\pi_l, \\mu_l, \\Sigma_l \\}_{l=1}^k}^* = \\theta^* = \\arg\\max_{\\theta} Q(\\Theta; \\Theta^t) = \\arg\\max_{\\theta}\\sum_{i=1}^n \\mathbb{E}_{z \\sim q_i^{*}(\\cdot)}\\left[\\log p(x_i, z|\\theta) \\right] $$\r\n",
        "First we express $Q(\\Theta; \\Theta^t)$ with our parameters.\r\n",
        "* Denote $ \\pi_j := p(z_j|\\theta) \\Longrightarrow \\sum_{j=1}^k \\pi_j = 1 $\r\n",
        "* Denote $ r_{ij} = p(z_i=j|x_i, \\theta) $. As we saw in class:\r\n",
        "$$ p({{z}_{i}}=j|{{x}_{i}},\\theta )=\\frac{p({{x}_{i}},{{z}_{i}}=j|\\theta )}{\\sum\\limits_{m}{p}({{x}_{i}},{{z}_{i}}=m|\\theta )}=\\frac{{{\\pi }_{j}}p({{x}_{i}}|{{\\mu }_{j}},{{\\Sigma }_{j}})}{\\sum\\limits_{m}{{{\\pi }_{m}}}p({{x}_{i}}|{{\\mu }_{m}},{{\\Sigma }_{m}})}=\\frac{{{\\pi }_{j}}|{{\\Sigma }_{j}}{{|}^{-\\frac{1}{2}}}{{e}^{-\\frac{1}{2}{{({{x}_{i}}-{{\\mu }_{j}})}^{T}}\\Sigma _{j}^{-1}({{x}_{i}}-{{\\mu }_{j}})}}}{\\sum\\limits_{m}{{{\\pi }_{m}}}|{{\\Sigma }_{m}}{{|}^{-\\frac{1}{2}}}{{e}^{-\\frac{1}{2}{{({{x}_{i}}-{{\\mu }_{m}})}^{T}}\\Sigma _{m}^{-1}({{x}_{i}}-{{\\mu }_{m}})}}} $$\r\n",
        "* As we saw in class:\r\n",
        "$$\r\n",
        "Q(\\Theta |{{\\Theta }^{t}})=\\sum\\limits_{i}{\\sum\\limits_{j=1}^{k}{{{r}_{ij}}}}\\log {{\\pi }_{j}}-\\frac{1}{2}\\sum\\limits_{j=1}^{k}{\\log }|{{\\Sigma }_{j}}|\\sum\\limits_{i}{{{r}_{ij}}}-\\frac{1}{2}\\sum\\limits_{i}{\\sum\\limits_{j=1}^{k}{{{r}_{ij}}}}{{({{x}_{i}}-{{\\mu }_{j}})}^{T}}\\Sigma _{j}^{-1}({{x}_{i}}-{{\\mu }_{j}})+Const\r\n",
        "$$\r\n",
        "Recall:\r\n",
        "$$ \\sum_i r_{ij}^{(t)} = \\lambda \\pi_j \\rightarrow \\sum_j \\sum_i r_{ij}^{(t)} = \\lambda \\sum_j \\pi_j \\rightarrow \\lambda = n\r\n",
        "$$\r\n",
        "Let's derive!\r\n",
        "$$\r\n",
        "\\frac{\\partial }{\\partial \\lambda }L(\\mu ,\\Sigma ,\\pi ,\\lambda )\\Rightarrow \\sum\\limits_{j}{{{\\pi }_{j}}}=1\r\n",
        "$$\r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{array}{*{35}{l}}\r\n",
        "   \\frac{\\partial }{\\partial {{\\pi }_{u}}}L(\\mu ,\\Sigma ,\\pi ,\\lambda ) & =\\frac{\\partial }{\\partial {{\\pi }_{u}}}Q(\\Theta ;{{\\Theta }^{t}})+\\lambda \\frac{\\partial }{\\partial {{\\pi }_{u}}}\\left\\{ 1-\\sum\\limits_{j}{{{\\pi }_{u}}} \\right\\}  \\\\\r\n",
        "   {} & \\begin{align}\r\n",
        "  & =\\frac{\\partial }{\\partial {{\\pi }_{u}}}\\left\\{ \\sum\\limits_{i}{{{r}_{iu}}}\\log {{\\pi }_{u}} \\right\\}-\\lambda  \\\\ \r\n",
        " & =\\sum\\limits_{i}{{{r}_{iu}}}\\frac{\\partial }{\\partial {{\\pi }_{u}}}\\left\\{ \\log {{\\pi }_{u}} \\right\\}-\\lambda  \\\\ \r\n",
        "\\end{align}  \\\\\r\n",
        "   {} & =\\sum\\limits_{i}{\\frac{{{r}_{iu}}}{{{\\pi }_{u}}}}-\\lambda   \\\\\r\n",
        "   {} & {}  \\\\\r\n",
        "\\end{array}\r\n",
        "$$\r\n",
        "After comparing to zero:\r\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\pi_j} = \\sum_i \\frac{r_{ij}}{\\pi_j} - \\lambda = \\sum_i \\frac{r_{ij}}{\\pi_j} - n = 0 $$\r\n",
        "$$ \\hat{\\pi}_j = \\frac{\\sum_{i=1}^n r_{ij}}{n} $$\r\n",
        "$$\r\n",
        "\\begin{align}\r\n",
        "\\frac{\\partial }{\\partial {{\\mu }_{u}}}L(\\mu ,\\Sigma ,\\pi ,\\lambda )=\\frac{\\partial }{\\partial {{\\mu }_{u}}}Q(\\Theta ;{{\\Theta }^{t}})&=\\frac{\\partial }{\\partial {{\\mu }_{u}}}\\left\\{ -\\frac{1}{2}\\sum\\limits_{i}{{{r}_{iu}}{{({{x}_{i}}-{{\\mu }_{u}})}^{T}}\\Sigma _{u}^{-1}({{x}_{i}}-{{\\mu }_{u}})} \\right\\} \\\\ \r\n",
        " & =-\\frac{1}{2}\\sum\\limits_{i}{{{r}_{iu}}\\frac{\\partial }{\\partial {{\\mu }_{u}}}\\left\\{ {{({{x}_{i}}-{{\\mu }_{u}})}^{T}}\\Sigma _{u}^{-1}({{x}_{i}}-{{\\mu }_{u}}) \\right\\}} \\\\ \r\n",
        " & =-\\frac{1}{2}\\sum\\limits_{i}{{{r}_{iu}}\\left\\{ -2{{I}^{T}}\\Sigma _{u}^{-1}({{x}_{i}}-I{{\\mu }_{u}}) \\right\\}} \\\\ \r\n",
        " & =\\sum\\limits_{i}{{{r}_{iu}}\\Sigma _{u}^{-1}({{x}_{i}}-{{\\mu }_{u}})} \\\\ \r\n",
        "\\end{align}\r\n",
        "$$\r\n",
        "After comparing to zero:\r\n",
        "$$\r\n",
        "\\begin{align}\r\n",
        "0&=\\sum\\limits_{i}{{{r}_{iu}}\\Sigma _{u}^{-1}({{x}_{i}}-{{\\mu }_{u}})} \\\\ \r\n",
        " & =\\Sigma _{u}^{-1}\\sum\\limits_{i}{\\left[ {{r}_{iu}}{{x}_{i}}-{{r}_{iu}}{{\\mu }_{u}} \\right]} \\\\ \r\n",
        " & =\\sum\\limits_{i}{{{r}_{iu}}{{x}_{i}}}-{{\\mu }_{u}}\\sum\\limits_{i}{{{r}_{iu}}} \\\\ \r\n",
        " & \\Rightarrow \\hat{{{\\mu }_{u}}}=\\frac{\\sum\\limits_{i}{{{r}_{iu}}{{x}_{i}}}}{\\sum\\limits_{i}{{{r}_{iu}}}} \\\\ \r\n",
        "\\end{align}\r\n",
        "$$\r\n",
        "* Similarly:\r\n",
        "$$ \\hat{\\Sigma}_u = \\frac{\\sum_{i} r_{iu}(x_i-\\mu_u)(x_i-\\mu_u)^T}{\\sum_{i} r_{iu}} $$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcABxk0WBMM"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ]
}