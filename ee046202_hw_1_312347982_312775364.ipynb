{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "ee046202_hw_1_312347982_312775364.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHtvj7Gyd07t"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
        "---\n",
        "\n",
        "## Homework 1 - Statistics\n",
        "---\n",
        "\n",
        "### <a style='color:red'> Due Date: 29.11.2020 </a>\n",
        "\n",
        "\n",
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "* Questions\n",
        "    * Gaussian RVs\n",
        "    * Parametric & Non-Parametric Estimation\n",
        "    * Exam Question - Estimators\n",
        "* Python Exercise - Parkinson's Disease Classification Data Analysis\n",
        "\n",
        "#### Use as many cells as you need\n",
        "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
        "\n",
        "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/000000/code.png\">\n",
        "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLLDfUsMd07v"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Gonen Weiss| gonen.weiss@campus.technion.ac.il| 312347982|\n",
        "|Alexander Balabanov| alexander.b@campus.technion.ac.il| 312775364|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMLqydc3d07w"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
        "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
        "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
        "* Submission only in **pairs**. \n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **ANSWERS TO THEORETICAL/MATHEMATICAL QUESTIONS**:\n",
        "    * **Typed - 5 points bonus**: you can type directly in a Markdown cell using Latex (see cheatsheets above), or use Word, Overleaf, LyX...\n",
        "        * This is a really good practice, we encourage you to practice your math typing skills.\n",
        "    * **Handwritten** - if we can't read your handwriting, we will automatically take off the points of the questions. Please write clearly. No bonus for handwritten submissions.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ee046202_hw1_id1_id2.ipynb`.\n",
        "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ee046202_hw1_id1_id2.zip` with content:\n",
        "        * `ee046202_hw1_id1_id2.ipynb` - the code tasks\n",
        "        * `ee046202_hw1_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKgjkz6vd07x"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046202-unsupervised-learning-data-analysis) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw1` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2_hk4k_d07x"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpgvjOBd07y"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - Gaussian RV- Basics\n",
        "---\n",
        "1. Let $Z \\sim \\mathcal{N}(0,1)$ be a normal Gaussian RV, and $X \\sim \\mathcal{N}(\\mu, \\sigma)$. The Cumulative Distribution Function (CDF) of $Z$ is defined as $$ P(Z\\leq c) \\triangleq \\phi(c).$$ Express $P(X\\geq x)$ using $\\phi(c)$.\n",
        "2. Consider a sequence of $N$ i.i.d. RVs $\\{X_i\\}_{i=1}^N$, where $X_i \\sim \\mathcal{N}(10,1)$. The empirical mean is given by $\\overline{X}_N = \\frac{1}{N}\\sum_{i=1}^NX_i$. What is the distribution of $\\overline{X}_N$?\n",
        "3. What is the probability $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$ for $N=1,10,20$? Express first using the function $\\phi(X)$ and then use `scipy.stats.norm.cdf` to calculate $\\phi(x)$ and obtain a numerical value.\n",
        "4. Since Gaussian RVs are not bounded, we cannot use **Hoeffding's** inequality to bound terms of the form  $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$. A possible alternative for this is to use the following proposition (which is more general and holds for a sum of sub-Gaussian RVs):\n",
        "    * **Proposition**: Let $\\{X_i\\}_{i=1}^N$ be i.i.d. RVs with $X_i \\sim \\mathcal{N}(\\mu, \\sigma)$.\n",
        "Then: $$ P(|\\frac{1}{N}\\sum_{i=1}^NX_i -\\mu|\\geq \\epsilon) \\leq 2 \\exp(-\\frac{N\\epsilon^2}{2\\sigma^2}).$$ Use this proposition to find a lower bound for $P(9.7 \\leq \\overline{X}_N\\leq 10.3)$ for $N=1,10,20$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2WbvZ6VhHkX"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution 1 - Gaussian RV- Basics\n",
        "---\n",
        "1. If we define $Z := \\frac{X - \\mu}{\\sigma}$ then $Z \\sim \\mathcal{N}(0,1)$ !  \n",
        "In that case: $ X = \\sigma Z + \\mu $\n",
        "\n",
        "\\begin{align} \n",
        "P(X \\leq x) &= P(\\sigma Z + \\mu \\leq x) \\\\\n",
        "&= P(Z \\leq \\frac{x - \\mu}{\\sigma}) \\\\\n",
        "&= \\phi(\\frac{x - \\mu}{\\sigma})\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align} \n",
        "P(X \\geq x) &= 1 - P(X \\leq x) \\\\\n",
        "&= 1 - \\phi(\\frac{x - \\mu}{\\sigma})\n",
        "\\end{align}\n",
        "\n",
        "2. Recall the following:\n",
        "    * A linear combination of i.i.d Gaussian RVs is also a Gaussian RV.\n",
        "    * Additonally, from the Law of Large Numbers (LLN) we know that:\n",
        "        * $ \\mathbb{E}[\\overline{X}_N] = \\mathbb{E}[X_i] = 10$\n",
        "        * $ Var[\\overline{X}_N] = \\frac{Var[X_i]}{N} = \\frac{1}{N} $\n",
        "* $ \\Longrightarrow \\overline{X}_N \\sim \\mathcal{N}(10,\\frac{1}{N}) $\n",
        "\n",
        "3. Recall that every CDF of an RV $R$ sustains:\n",
        "$ P(a \\leq R \\leq b) = P(R \\leq b) - P(R \\leq a) = F_R(b) - F_R(a) $ <br>\n",
        "* In our case:\n",
        "\n",
        "\\begin{align} \n",
        "P(9.7 \\leq \\overline{X}_N\\leq 10.3) &= \n",
        "    P(\\overline{X}_N \\leq 10.3) - P(\\overline{X}_N \\leq 9.7) \\\\\n",
        "&\\stackrel{(1)}{=} \\phi(\\frac{10.3 - 10}{\\sqrt{\\frac{1}{N}}}) - \n",
        "   \\phi(\\frac{9.7 - 10}{\\sqrt{\\frac{1}{N}}}) \\\\\n",
        "&= \\phi(0.3\\sqrt{N}) - \\phi(-0.3\\sqrt{N})\n",
        "\\end{align}\n",
        "\n",
        "* Using `scipy.stats.norm.cdf` we get (see code below):\n",
        "    * $ P(9.7 \\leq \\overline{X}_{1}\\leq 10.3) = 0.236 $\n",
        "    * $ P(9.7 \\leq \\overline{X}_{10}\\leq 10.3) = 0.657 $\n",
        "    * $ P(9.7 \\leq \\overline{X}_{20}\\leq 10.3) = 0.820 $\n",
        "\n",
        "4. In our case:\n",
        "* $ X_i \\sim \\mathcal{N}(10,1) \\Rightarrow \\mu = 10, \\sigma = 1 $\n",
        "* $ \\overline{X}_N = \\frac{1}{N}\\sum_{i=1}^NX_i $\n",
        "* We shall apply the proposition with $\\epsilon=0.3$. Recall:\n",
        "    * Firstly:\n",
        "\\begin{align} \n",
        "P(|\\frac{1}{N}\\sum_{i=1}^NX_i -\\mu|\\geq \\epsilon)\n",
        "&= P(|\\overline{X}_N - 10| \\geq 0.3) \\\\\n",
        "&= 1 - P(|\\overline{X}_N - 10| \\leq 0.3) \\\\\n",
        "&= 1 - P(9.7 \\leq \\overline{X}_N \\leq 10.3)\n",
        "\\end{align}\n",
        "    * Secondly:\n",
        "\\begin{align}\n",
        "2\\exp(-\\frac{N\\epsilon^2}{2\\sigma^2})\n",
        "&= 2\\exp(-\\frac{0.3^2N}{2\\cdot1^2}) \\\\\n",
        "&= 2\\exp(-0.045N)\n",
        "\\end{align}\n",
        "    * Thirdly, the proposition sustains:\n",
        "\\begin{align}\n",
        "P(|\\frac{1}{N}\\sum_{i=1}^NX_i -\\mu|\\geq \\epsilon) \\leq \n",
        "2\\exp(-\\frac{N\\epsilon^2}{2\\sigma^2})\n",
        "\\end{align}\n",
        "    * Thus:\n",
        "\\begin{align}\n",
        "1 - P(9.7 \\leq \\overline{X}_N \\leq 10.3) &\\leq 2\\exp(-0.045N) \\\\\n",
        "P(9.7 \\leq \\overline{X}_N \\leq 10.3) &\\geq 1 - 2\\exp(-0.045N)\n",
        "\\end{align}\n",
        "    * This yields:\n",
        "        * $ P(9.7 \\leq \\overline{X}_{1}\\leq 10.3) \\geq -0.912 $\n",
        "        * $ P(9.7 \\leq \\overline{X}_{10}\\leq 10.3) \\geq -0.275 $\n",
        "        * $ P(9.7 \\leq \\overline{X}_{20}\\leq 10.3) \\geq  0.187 $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wAW9GKx_mLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b00653-f11a-4779-8934-a6df3221c263"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from scipy.stats  import norm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib notebook\n",
        "\n",
        "# 3. \n",
        "N = np.array([1, 10, 20])\n",
        "P = norm.cdf(0.3 * np.sqrt(N)) - norm.cdf(-0.3 * np.sqrt(N))\n",
        "print(f\"Exact probabilities: {P}\")\n",
        "\n",
        "# 4. \n",
        "P_LB = 1 - 2 * np.exp(-0.045 * N)\n",
        "print(f\"Lower bounds: {P_LB}\")\n",
        "print(f\"Difference: {P - P_LB}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact probabilities: [0.23582284 0.65721829 0.82028751]\n",
            "Lower bounds: [-0.91199496 -0.2752563   0.18686068]\n",
            "Difference: [1.14781781 0.93247459 0.63342682]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT_jl9zWd07z"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Parametric and Non-Parametric Estimation\n",
        "---\n",
        "1. Suppose $\\hat{\\theta}$ is an estimator for an unknown parameter $\\theta$. Show that $$MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + Bias^2(\\hat{\\theta})$$\n",
        "2. Let $X_1, ..., X_N \\sim Bernoulli(p)$ and let $Y_1, ..., Y_N \\sim Bernoulli(q)$ be i.i.d. RVs. \n",
        "    * Find a (non-parametric) point estimator and the estimated standard error for $p$ (use Hoeffding).\n",
        "        * Recall the standard deviation: $$ s_N(\\hat{\\theta}) = \\sqrt{Var(\\hat{\\theta}}) $$\n",
        "    * Find an approximated 90%, 95%, 99% confidence intervals for $p$. \n",
        "    * Find the point estimator and the estimated standard error for $p-q$.\n",
        "    * Find an approximated 90% confidence interval for $p-q$.\n",
        "3. Let $X_1, ..., X_N \\sim Binomial(10, \\theta)$ be i.i.d. RVs. Estimate $\\theta$ using MLE.\n",
        "    * $P(X_i|\\theta) = \\begin{pmatrix}10 \\\\ X_i \\end{pmatrix} \\theta^{X_i}(1-\\theta)^{10-X_i}$\n",
        "4. Let $X_1, ..., X_N \\sim F$ be i.i.d. RVs, where $F$ is an arbitrary, unknown CDF. Let $\\hat{F}$ be the empirical distribution function. For a fixed $F$, use the central limit theorem (CLT) to find the limiting distribution of $\\hat{F}_n(x)$.\n",
        "5. In the lecture and tutorial, we stated the **DKW** theorem and derived a C.I. for the empirical CDF, for 1D type of data. Derive a C.I. for the empirical CDF in a general dimension, i.e., as a function of $C(k)$[see non-parametric chapter in the lectures]. If $C(k) \\sim \\exp(k)$, what does it mean about the *hardness* of the problem in high dimension?\n",
        "6. Calculate $\\mathbb{E}[\\hat{F}_N]$ and $Var[\\hat{F}_N]$ using the definition of the empirical distribution function (remember that $X_1,..., X_N$ are i.i.d. from $F$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvCbmdRoI4hP"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution 2 - Parametric and Non-Parametric Estimation\n",
        "---\n",
        "1. Recall that:\n",
        "* $ MSE(\\hat{\\theta}) \\triangleq \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] $\n",
        "* $ Var(\\hat{\\theta}) \\triangleq \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] $\n",
        "* $ Bias(\\hat{\\theta}) \\triangleq \\mathbb{E}[\\hat{\\theta}] - \\theta $\n",
        "* Note: bare $\\hat{\\theta}$ is the only RV in each of the terms!\n",
        "<br> All the other terms, including $ \\theta, \\mathbb{E}[\\hat{\\theta}], Bias(\\hat{\\theta}), Var(\\hat{\\theta}), MSE(\\hat{\\theta}) $ are deterministic!\n",
        "\n",
        "\\begin{align} \n",
        "MSE(\\hat{\\theta}) &= \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] \\\\\n",
        "&= \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]\n",
        "+ \\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\\n",
        "&= \\mathbb{E}[\\big((\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])\n",
        "+ (\\mathbb{E}[\\hat{\\theta}] - \\theta)\\big)^2] \\\\\n",
        "&= \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2\n",
        "+ 2(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}] - \\theta)\n",
        "+ (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\\n",
        "&= \\mathbb{E}\\Big[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2\\Big]\n",
        "+ \\mathbb{E}\\Big[2(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}] - \\theta)\\Big] \n",
        "+ \\mathbb{E}\\Big[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2\\Big]\\\\\n",
        "&= Var(\\hat{\\theta})\n",
        "+ \\mathbb{E}\\Big[2(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])Bias(\\hat{\\theta})\\Big] \n",
        "+ \\mathbb{E}\\Big[Bias(\\hat{\\theta})^2\\Big]\\\\\n",
        "&= Var(\\hat{\\theta})\n",
        "+ 2Bias(\\hat{\\theta})\\mathbb{E}\\Big[\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]\\Big] \n",
        "+ Bias(\\hat{\\theta})^2\\\\\n",
        "&= Var(\\hat{\\theta})\n",
        "+ 2Bias(\\hat{\\theta})(\\mathbb{E}[\\hat{\\theta}] - \\mathbb{E}[\\hat{\\theta}]) \n",
        "+ Bias(\\hat{\\theta})^2\\\\\n",
        "&= Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2\\\\\n",
        "\\end{align}\n",
        "\n",
        "2. If $ X \\sim Bernoulli(p) $ is an RV of the Bernoulli distribution, then it is a Counting Variable:\n",
        "$$ X: \\Omega \\rightarrow \\{0,1\\} \\subseteq \\mathbb{N}  $$\n",
        "Thus if we knew the CDF of $X$, we could have applied the Tail Sum Formula. This yields: \n",
        "$$ \\mathbb{E}[X] = \\sum_{j=1}^1 P(X \\geq j) = P(X = 1) \\triangleq p $$\n",
        "Unsurprisingly we got $ p = \\mathbb{E}[X] $, which means that in order to point-estimate $p$ we should point-estimate the expectation of $X$. We know that it can be done using the Sample Mean $ \\overline{X} $, and it is indeed a non-parametric estimator (it comes directly from the samples).\n",
        "<br>We will estimate it in another approach, more CDF-based (PMF in our case).\n",
        "<br>Since we are in a non-parametric approach, we don't have the PMF of $X$. We can estimate it using the Empirical PMF:\n",
        "$$ P_X(x) \\triangleq P(X = x) \\Longrightarrow \\hat{P_X}(x) := \\frac{1}{N}\\sum_{i=1}^N I_{\\{X_i = x\\}} $$\n",
        "As noted before, $ p \\triangleq P(X = 1) $, thus we can estimate $p$ by:\n",
        "$$ \\hat{p} := \\hat{P_X}(1) = \\frac{1}{N}\\sum_{i=1}^N I_{\\{X_i = 1\\}} = \\frac{1}{N}\\sum_{i=1}^N X_i = \\overline{X} $$\n",
        "(We used the fact that both $X$ and $ I_{\\{X = 1\\}} $ equal\n",
        "$\n",
        "\\begin{cases} \n",
        "0, & X = 0 \\\\\n",
        "1, & X = 1 \\\\\n",
        "\\end{cases}\n",
        "$)\n",
        "<br><br>To conclude, via a non-parametric approach we have found a point estimator for $p$:\n",
        "$$ \\hat{p} := \\overline{X} = \\frac{1}{N}\\sum_{i=1}^N X_i $$\n",
        "<br>Next, proceeding to Standard Error (SE):\n",
        "\\begin{align}\n",
        "\\mathbb{E}[X^2] &= 0^2 \\cdot \\hat{P_X}(0) + 1^2 \\cdot \\hat{P_X}(1) = \\hat{P_X}(1) = \\overline{X}\\\\\n",
        "Var[X] &= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2 = \\overline{X} - \\overline{X}^2 = \\overline{X}(1 - \\overline{X}) \\\\\n",
        "Var[\\hat{p}] &= Var[\\overline{X}]\n",
        "= Var[\\frac{1}{N}\\sum_{i=1}^N X_i]\n",
        "= \\frac{1}{N^2}\\sum_{i=1}^N Var[X_i]\n",
        "= \\frac{N}{N^2} Var[X] = \\frac{\\overline{X}(1 - \\overline{X})}{N} \\\\\n",
        "\\Longrightarrow s_N(\\hat{p}) &\\triangleq \\sqrt{Var(\\hat{p}}) = \\sqrt{\\frac{\\overline{X}(1 - \\overline{X})}{N}}\n",
        "\\end{align}\n",
        "\n",
        "* Recall that $\\hat{p}$ is unbiased:\n",
        "$$ \\mathbb{E}[\\hat{p}] = \\mathbb{E}[\\overline{X}] \\stackrel{\\scriptsize{\\overline{X} unbiased}}{=} \\mathbb{E}[X] = p $$We would like to find boundaries for $p$ around $\\hat{p}$ with confidence level of $ 1 - \\alpha $:\n",
        "$$ P(|p - \\hat{p}| < \\epsilon) \\stackrel{!}{=} 1 - \\alpha $$\n",
        "Hoeffding can be applied here with $ b_i - a_i = 1 $ because $ \\forall i: X_i \\in \\{0,1\\} \\Longrightarrow \\overline{X} \\in [0,1] $\n",
        "<br><br>This yields:\n",
        "\\begin{align}\n",
        "\\alpha \\stackrel{!}{=} 1 - P(|p - \\hat{p}| < \\epsilon) &= 1 - P(|\\overline{X} - \\mathbb{E}[\\overline{X}]| < \\epsilon) \\\\\n",
        "&= P(|\\overline{X} - \\mathbb{E}[\\overline{X}]| \\geq \\epsilon) \\\\\n",
        "\\scriptsize{Hoeffding \\rightarrow} \\quad &\\leq 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(a_i - b_i)^2}} \\\\\n",
        "&= 2e^{-2N\\epsilon^2}\n",
        "\\end{align}\n",
        "Therefore, by taking $ \\epsilon := \\sqrt{\\frac{ln(\\frac{2}{\\alpha})}{2N}} $ we get that the true value of $p$ is in $ [\\hat{p} - \\epsilon, \\hat{p} + \\epsilon] $ with probability of $ 1 - \\alpha $.\n",
        "    * Confidence level of 90% ($\\alpha=0.1$) yields $ p \\in [\\hat{p} - \\frac{1.22}{\\sqrt{N}}, \\hat{p} + \\frac{1.22}{\\sqrt{N}}] $\n",
        "    * Confidence level of 95% ($\\alpha=0.05$) yields $ p \\in [\\hat{p} - \\frac{1.36}{\\sqrt{N}}, \\hat{p} + \\frac{1.36}{\\sqrt{N}}] $\n",
        "    * Confidence level of 99% ($\\alpha=0.01$) yields $ p \\in [\\hat{p} - \\frac{1.63}{\\sqrt{N}}, \\hat{p} + \\frac{1.63}{\\sqrt{N}}] $\n",
        "\n",
        "* Like before, $ p = \\mathbb{E}[X] $, $ q = \\mathbb{E}[Y] $, thus:\n",
        "$$ p - q = \\mathbb{E}[X] - \\mathbb{E}[Y] \\Rightarrow \\widehat{p - q} := \\overline{X} - \\overline{Y} $$\n",
        "<br>Lets evaluate its statistics:<br>\n",
        "$ \\mathbb{E}[\\widehat{p-q}] = \\mathbb{E}[\\overline{X} - \\overline{Y}] = \\mathbb{E}[\\overline{X}] - \\mathbb{E}[\\overline{Y}] = p - q $\n",
        "<br>$ \\Longrightarrow $ Unbiased estimator!\n",
        "<br>$ Var[\\widehat{p-q}] = Var[\\overline{X} - \\overline{Y}] = Var[\\overline{X}] + Var[-\\overline{Y}] = Var[\\overline{X}] + (-1)^2Var[\\overline{Y}] = \\frac{\\overline{X}(1-\\overline{X})}{N} + \\frac{\\overline{Y}(1-\\overline{Y})}{N} $\n",
        "<br><br>Thus:<br>\n",
        "$ s_N(\\widehat{p-q}) = \\sqrt{\\frac{\\overline{X}(1-\\overline{X}) + \\overline{Y}(1-\\overline{Y})}{N}} $\n",
        "\n",
        "* Recall that $\\widehat{p-q}$ is unbiased.\n",
        "We would like to find boundaries for $ p - q $ around $\\widehat{p-q}$ with confidence level of $ 1 - \\alpha $:\n",
        "$$ P(|(p-q) - (\\widehat{p-q})| < \\epsilon) \\stackrel{!}{=} 1 - \\alpha $$\n",
        "Hoeffding can be applied here with $ b_i - a_i = 2 $ because<br>\n",
        "$ \\forall i: X_i \\in \\{0,1\\} \\Longrightarrow \\overline{X} \\in [0,1] $<br>\n",
        "$ \\forall i: Y_i \\in \\{0,1\\} \\Longrightarrow \\overline{Y} \\in [0,1] $<br>\n",
        "$ \\Rightarrow \\overline{X} - \\overline{Y} \\in [-1,1] $\n",
        "<br><br>This yields:\n",
        "\\begin{align}\n",
        "\\alpha \\stackrel{!}{=} 1 - P(|(p-q) - (\\widehat{p-q})| < \\epsilon) &= 1 - P(|(\\overline{X} - \\overline{Y}) - \\mathbb{E}[(\\overline{X} - \\overline{Y})]| < \\epsilon) \\\\\n",
        "&= P(|(\\overline{X} - \\overline{Y}) - \\mathbb{E}[(\\overline{X} - \\overline{Y})]| \\geq \\epsilon) \\\\\n",
        "\\scriptsize{Hoeffding \\rightarrow} \\quad &\\leq 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(a_i - b_i)^2}} \\\\\n",
        "&= 2e^{-\\frac{1}{2}N\\epsilon^2}\n",
        "\\end{align}\n",
        "Therefore, by taking $ \\epsilon := \\sqrt{\\frac{2ln(\\frac{2}{\\alpha})}{N}} $ we get that the true value of $p-q$ is in $[\\widehat{p-q} - \\epsilon, \\widehat{p-q} + \\epsilon]$ with probability of $ 1 - \\alpha $.\n",
        "    * Confidence level of 90% ($\\alpha=0.1$) yields $ p - q \\in [\\widehat{p-q} - \\frac{2.44}{\\sqrt{N}}, \\widehat{p-q} + \\frac{2.44}{\\sqrt{N}}] $\n",
        "    <br>Twice bigger than $p$'s alone!\n",
        "<br><br>\n",
        "\n",
        "3. To estimate the parameter $\\theta$ using MLE we should find the maximizing $\\theta$ of the log-likelyhood function:\n",
        "* The parameters are:\n",
        "    * $\\theta$ - The probability of each successful trial.\n",
        "* $ P(X_i|\\theta) = \\begin{pmatrix}10 \\\\ X_i \\end{pmatrix} \\theta^{X_i}(1-\\theta)^{10-X_i} := C_i \\theta^{X_i}(1-\\theta)^{10-X_i} $\n",
        "    * $C_i$ does not depend on $\\theta$ (and it will be gone when we derive the log-likelehood function)\n",
        "* $ L(\\theta) = p(X_1, X_2, ..., X_n |\\theta) \\stackrel{iid}{=} \\prod_{i=1}^N p(X_i|\\theta) = C_{1,..,N} \\cdot \\theta^{\\sum_{i=1}^N X_i} (1-\\theta)^{\\sum_{i=1}^N {(10-X_i)}}$\n",
        "* $ l(\\theta) = \\ln L(\\theta) = lnC_{1,..,N} +  ln(\\theta) \\cdot {\\sum_{i=1}^N X_i} + ln(1-\\theta) \\cdot {\\sum_{i=1}^N {(10-X_i)}} $\n",
        "* Now we derive with respect to $\\theta$ and find zero derivatives:\n",
        "\\begin{align}\n",
        "0 &\\stackrel{!}{=} \\frac{dl(\\theta)}{d\\theta} = \\frac{1}{\\theta}\\cdot {\\sum_{i=1}^N X_i} + \\frac{-1}{1-\\theta}{\\sum_{i=1}^N {(10-X_i)}} \\\\\n",
        "\\frac{1}{\\theta}\\cdot{\\sum_{i=1}^N X_i} &\\stackrel{!}{=} \\frac{1}{1-\\theta}{\\sum_{i=1}^N (10-X_i)} = \\frac{1}{1-\\theta}(10N - {\\sum_{i=1}^N X_i}) \\\\\n",
        "(\\frac{1}{\\theta}-1)\\cdot{\\sum_{i=1}^N X_i} &\\stackrel{!}{=}10N - {\\sum_{i=1}^N X_i} \\\\\n",
        "\\hat{\\theta}_{MLE} &\\stackrel{!}{=} \\frac{1}{10N} {\\sum_{i=1}^N X_i} = \\frac{1}{10}\\overline{X}\\\\\n",
        "\\end{align}\n",
        "    * This has to be a maximum, because $\\theta=0$ and $\\theta=1$ both minimize the likelyhood to be zero (assuming not all $X_i$ equal only $0$s or only $1$s)\n",
        "    * This makes sense because if $ X \\sim Binomial(n,p) $ and $n$ is known then $ \\overline{X} = \\widehat{\\mathbb{E}[X]} = \\widehat{np} \\Longrightarrow \\hat{p} = \\frac{1}{n}\\overline{X} $\n",
        "\n",
        "4. \n",
        "* Firstly, about Bernoulli RVs. Suppose $ Y \\sim Bernoulli(p) $, then:\n",
        "    * $ \\mathbb{E}[Y] = p $\n",
        "    * $ \\mathbb{E}[Y^2] = p $\n",
        "    * $ Var[Y] = \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2 = p(1-p) $\n",
        "* Let there be $ x \\in \\mathbb{R} $. Define $ Y_i(x) := \\mathbb{1}_{\\{X_i \\leq x\\}} $. <br>\n",
        "$ Y_i \\sim Bernoulli\\big(F(x)\\big) $ because $ P(Y_i = 1) = P(X_i \\leq x) = F(x) $\n",
        "Hence all the above properties of Bernoulli distribution hold for $Y_i$ too.\n",
        "* Recall the Empirical CDF (ECDF):\n",
        "    * $ \\hat{F}_N(x) \\triangleq \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{\\{X_i \\leq x\\}} = \\overline{Y(x)} $\n",
        "    * As an empirical mean, it converges into its expectation with decaying variance over $N$ (thanks to the CLT, since an empirical mean is an unbiased estimator).<br>\n",
        "$ \\lim_{N \\to \\infty} \\hat{F}_N(x) = \\lim_{N \\to \\infty} \\overline{Y(x)} = \\mathbb{E}[Y(x)] = \\mathbb{E}[\\mathbb{1}_{\\{X \\leq x\\}}] = F(x) $\n",
        "    * More specifically, according to CLT and the properties of Bernoulli RVs:<br>\n",
        "     $ \\frac{\\hat{F}_N(x) - F(x)}{\\sqrt{\\frac{F(x)(1-F(x))}{N}}} \\stackrel{N \\to \\infty}{\\sim} \\mathcal{N}(0,1) $\n",
        "\n",
        "5. \n",
        "* Recall the DKW theorem for the 1D case, $ X_1,...X_N \\sim F = F(x), x \\in \\mathbb{R} $:\n",
        "$$ Pr\\big( \\underset{x \\in \\mathbb{R}}{sup} |F_N(x) - F(x)| > \\epsilon \\big) \\leq 2e^{-2N\\epsilon^2}, \\forall \\epsilon >0 $$\n",
        "This yields a confidence interval for the true CDF $F(x)$:\n",
        "$ \\forall x \\in \\mathbb{R}: F(x) \\in [F_N(x) - \\epsilon_N, F_N(x) + \\epsilon_N]  $ with confidence level $ 1 - \\alpha $, where:<br>\n",
        "$$ \\alpha = 2e^{-2N{\\epsilon}^2} \\Rightarrow \\epsilon_N := \\sqrt{\\frac{ln(\\frac{2}{\\alpha})}{2N}} $$\n",
        "Note: $\\epsilon_N$ is **independent** of $x$\n",
        "* In the lecture we were given the DKW theorem for the k-dimensional case, $ \\underline{X}_1,...\\underline{X}_N \\sim F = F(\\underline{x}), \\underline{x} \\in \\mathbb{R}^k $:\n",
        "$$ Pr\\big( \\underset{\\underline{x} \\in \\mathbb{R}^k}{sup} |F_N(\\underline{x}) - F(\\underline{x})| > \\epsilon \\big) \\leq C(k)e^{-2N\\epsilon^2}, \\forall \\epsilon >0 $$\n",
        "This yields a confidence interval for the true CDF $F(\\underline{x})$:\n",
        "$ F(\\underline{x}) \\in [F_N(\\underline{x}) - \\epsilon_N, F_N(\\underline{x}) + \\epsilon_N]  $ with confidence level $ 1 - \\alpha $, where:<br>\n",
        "$$ \\alpha = C(k)e^{-2N{\\epsilon}^2} \\Rightarrow \\epsilon_N := \\sqrt{\\frac{ln(\\frac{C(k)}{\\alpha})}{2N}} $$\n",
        "Note again: $\\epsilon_N$ is **independent** of $\\underline{x}$\n",
        "* Recall that usually with confidence intervals, the more samples we have, the tighter the interval is for the same confidence level. For a tightness of $\\pm\\epsilon$, $ N = \\frac{1}{2\\epsilon^2}ln(2) - \\frac{1}{2\\epsilon^2}ln(\\alpha) \\Longrightarrow N \\propto -ln(\\alpha) $.<br>\n",
        "This means that in order for **the same interval** to produce a ~2.78x lower probability of an error, additional $ \\frac{1}{\\epsilon^2} $ samples are all it takes. This could be large, however constant number!<br>\n",
        "On the other hand, if $ C(k) \\propto e^k $, then $ \\epsilon_N \\approx \\sqrt{\\frac{k + ln(\\frac{1}{\\alpha})}{2N}} $.<br>\n",
        "Thus for a tightness of $\\pm\\epsilon$, $ N = \\frac{1}{2\\epsilon^2}k - \\frac{1}{2\\epsilon^2}ln(\\alpha) \\Longrightarrow N \\propto k-ln(\\alpha) $.<br>\n",
        "This result is an instantiation of the Curse Of Dimensionality. It means that $ \\textbf{ln}(\\alpha) $ and $k$ have the same effect on the amount of required samples, i.e the same amount of additional samples required to produce a ~2.78x lower probability of an error is required to increase dimension by one.\n",
        "\n",
        "6. This subsection is pretty much the same as subsection (2) and (4).\n",
        "* Recall the Empirical CDF (ECDF):\n",
        "$$ \\hat{F}_N(x) \\triangleq \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{\\{X_i \\leq x\\}} $$\n",
        "* Additionally note that $ (\\mathbb{1}_{\\{X_i \\leq x\\}})^2 \\equiv \\mathbb{1}_{\\{X_i \\leq x\\}} $\n",
        "* Expectation:\n",
        "\\begin{align}\n",
        "\\mathbb{E}[\\hat{F}_N(x)] &= \\mathbb{E}[\\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{\\{X_i \\leq x\\}}] \\\\\n",
        "&= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}[\\mathbb{1}_{\\{X_i \\leq x\\}}] \\\\\n",
        "&= \\frac{1}{N} \\sum_{i=1}^N P(X_i \\leq x) \\\\\n",
        "&\\stackrel{i.i.d}{=} \\frac{N}{N}F(x) \\\\\n",
        "&= F(x)\n",
        "\\end{align}\n",
        "* Variance:\n",
        "\\begin{align}\n",
        "Var[\\hat{F}_N(x)] &= Var[\\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{\\{X_i \\leq x\\}}] \\\\\n",
        "&\\stackrel{i.i.d}{=} \\frac{1}{N^2} \\sum_{i=1}^N Var[\\mathbb{1}_{\\{X \\leq x\\}}] \\\\\n",
        "&= \\frac{1}{N^2} \\sum_{i=1}^N \\big(\\mathbb{E}[(\\mathbb{1}_{\\{X \\leq x\\}})^2] - \\mathbb{E}[\\mathbb{1}_{\\{X \\leq x\\}}]^2\\big) \\\\\n",
        "&= \\frac{1}{N^2} \\sum_{i=1}^N \\big(\\mathbb{E}[\\mathbb{1}_{\\{X \\leq x\\}}] - \\mathbb{E}[\\mathbb{1}_{\\{X \\leq x\\}}]^2\\big) \\\\\n",
        "&= \\frac{N}{N^2}\\big(F(x)-F(x)^2\\big) \\\\\n",
        "&= \\frac{1}{N}F(x)(1-F(x))\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivkEfRV0ErvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5184904f-6505-4444-be85-d2ceb575cf3e"
      },
      "source": [
        "\n",
        "# imports\n",
        "import numpy as np\n",
        "\n",
        "# 2.\n",
        "alpha = np.array([0.1, 0.05, 0.01])\n",
        "epsilon_prod_sqrtN = np.sqrt(np.log(2/alpha) / 2)\n",
        "print(f\"Epsilons corresponding to alphas {alpha} are {epsilon_prod_sqrtN} / sqrt(N)\")\n",
        "\n",
        "# 3.\n",
        "alpha = np.array([0.1])\n",
        "epsilon_prod_sqrtN = np.sqrt(2*np.log(2/alpha))\n",
        "print(f\"Epsilons corresponding to alphas {alpha} are {epsilon_prod_sqrtN} / sqrt(N)\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilons corresponding to alphas [0.1  0.05 0.01] are [1.22387342 1.35810152 1.62762363] / sqrt(N)\n",
            "Epsilons corresponding to alphas [0.1] are [2.44774683] / sqrt(N)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAbLtigd07z"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - Exam Question - Estimators\n",
        "---\n",
        "In order to check electrical devices, a system performs repeated tests in a device until its first failure appears. The tests were performed on $N$ devices. Denote $K_i$ as the number if tests that were performed on the $i^{th}$ device (including the final test where the failure appeared), where $i \\in \\{1,...,N\\}$. Assume that $K_1,...,K_N$ are i.i.d. random variables.\n",
        "1. Find a non-parametric estimator(i.e. the plug-in/point estimator) for $\\mathbb{E}[K]$ (hint: use the Tail Sum formula).\n",
        "2. Find a non-parametric estimator for $\\hat{p}_3 \\triangleq P(K=3)$, the probability that a failure will occur in the third test.\n",
        "3. Suggest a confidence interval (CI) for $\\hat{p}_3$ for $\\alpha = 0.05, N=100$.\n",
        "\n",
        "Assume $K \\sim Geom(p)$ (Geometric Distribution), where $p$ is unknown.\n",
        "4. Calculate the MLE for $p$ and the mean $\\mu = \\mathbb{E}[K]$.\n",
        "5. Calculate the probability that the number of tests is odd, i.e., that $K$ is odd, $P(K \\text{ is odd})$. Simplify as much as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXPOo7SGvLoU"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution 3 - Exam Question - Estimators\n",
        "---\n",
        "1. Recall that an estimation of probabilities can be done through empirical CDFs (and PMFs), which is achieved from indicators.\n",
        "$$ P(K \\geq k) = \\mathbb{E}[I_{\\{K \\geq k\\}}] \\Rightarrow \\widehat{P(K \\geq k)} = \\widehat{\\mathbb{E}[I_{\\{K \\geq k\\}}]} = \\overline{I_{\\{K \\geq k\\}}} = \\frac{1}{N} \\sum_{i=1}^N I_{\\{K_i \\geq k\\}} $$\n",
        "Using Tail-Sum-Formula:\n",
        "\\begin{align}\n",
        "\\mathbb{E}[K] = \\sum_{k=1}^\\infty P(K_i \\geq k) \\Longrightarrow \\\\\n",
        "\\widehat{\\mathbb{E}[K]} &= \\sum_{k=1}^\\infty \\widehat{P(K_i \\geq k)} \\\\\n",
        "&= \\sum_{k=1}^\\infty \\frac{1}{N} \\sum_{i=1}^N I_{\\{K_i \\geq k\\}} \\\\\n",
        "&= \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^\\infty I_{\\{K_i \\geq k\\}} \\\\\n",
        "&= \\frac{1}{N} \\sum_{i=1}^N K_i \\\\\n",
        "&= \\overline{K}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "2. Again, we can estimate PMF using indicators:\n",
        "$$ \\hat{p}_3 \\triangleq \\widehat{P(K=3)} = \\frac{1}{N} \\sum_{i=1}^N I_{\\{K_i = 3\\}} = \\overline{I_{\\{K = k\\}}} $$\n",
        "Knowing that $ K \\sim Geom(p) $, this doesn't look like a good estimator, but for a non-parametric model it is the best we can do.\n",
        "\n",
        "\n",
        "3. Our $ \\hat{p}_3 = \\overline{I_{\\{K = k\\}}} $ is an unbiased estimator bounded by [0,1] (as the sample-mean of indicators). Thus, a CI can be calculated using Hoeffding. For $\\alpha = 0.05, N=100$:\n",
        "\\begin{align}\n",
        "\\alpha \\stackrel{!}{=} 1 - P(|p_3 - \\hat{p}_3| < \\epsilon)\n",
        "&= 1 - P(|\\overline{I_{\\{K = k\\}}} - \\mathbb{E}[\\overline{I_{\\{K = k\\}}}]| < \\epsilon) \\\\\n",
        "&= P(|\\overline{I_{\\{K = k\\}}} - \\mathbb{E}[\\overline{I_{\\{K = k\\}}}]| \\geq \\epsilon) \\\\\n",
        "\\scriptsize{Hoeffding \\rightarrow} \\quad &\\leq 2e^{-\\frac{2N^2\\epsilon^2}{\\sum_{i=1}^N(a_i - b_i)^2}} \\\\\n",
        "&= 2e^{-2N\\epsilon^2}\n",
        "\\end{align}\n",
        "Therefore, by taking $ \\epsilon := \\sqrt{\\frac{ln(\\frac{2}{\\alpha})}{2N}} $ we get that the true value of $p_3$ is in $ [\\hat{p}_3 - \\epsilon, \\hat{p}_3 + \\epsilon] $ with probability of $ 1 - \\alpha $. In our case:\n",
        "    * Confidence level of 95% ($\\alpha=0.05$) gives $ \\epsilon = \\sqrt{\\frac{ln(\\frac{2}{0.05})}{2 \\cdot 100}}  = 0.136 $ which yields a CI of $ p_3 \\in [\\hat{p}_3 - 0.136, \\hat{p}_3 + 0.136] $\n",
        "\n",
        "\n",
        "4. Regarding Geometric Distribution:\n",
        "* $ K \\sim Geom(p) $\n",
        "* $ P(K=k) = (1-p)^{k-1}p $\n",
        "* $ \\Rightarrow logP(K=k) = (k-1)log(1-p) + log(p) $\n",
        "* $ L(p) = \\prod_{i=1}^N P(K_i; p) $\n",
        "* $ \\Rightarrow l(p) = \\sum_{i=1}^N logP(K_i; p) $\n",
        "<br>\n",
        "Start from the mean $ \\mu = \\mathbb{E}[K] $ (using Tail-Sum-Formula):\n",
        "* Note that $ \\{K \\geq k\\} = \\{K = k\\} \\cup \\{K = k+1\\} \\cup \\{...\\} = \\big\\{\\text{all k-1 first attempts were successful}\\big\\} $\n",
        "* Hence $ P\\{K \\geq k\\} = P\\big\\{\\text{all k-1 first attempts were successful}\\big\\} = (1-p)^{k-1} $\n",
        "\\begin{align}\n",
        "\\mu = \\mathbb{E}[K] &= \\sum_{k=1}^\\infty P(K \\geq k) \\\\\n",
        "&= \\sum_{k=1}^\\infty (1-p)^{k-1} \\\\\n",
        "&= \\sum_{k=0}^\\infty (1-p)^{k} \\\\\n",
        "&= \\frac{1}{1 - (1-p)} \\\\\n",
        "&= \\frac{1}{p}\n",
        "\\end{align}\n",
        "<br>\n",
        "Now for the MLE of $p$:\n",
        "\\begin{align}\n",
        "\\hat{p}_{MLE} &= \\underset{p}{argmax} \\big\\{ l(p) \\big\\} \\\\\n",
        "&= \\underset{p}{argmax} \\big\\{\\sum_{i=1}^N \\big((K_i-1)log(1-p) + log(p) \\big)\\big\\} \\\\\n",
        "&= \\underset{p}{argmax} \\big\\{\\sum_{i=1}^N \\big(K_i log(1-p) + log(\\frac{p}{1-p}) \\big)\\big\\} \\\\\n",
        "&= \\underset{p}{argmax} \\big\\{\\sum_{i=1}^N \\big(K_i \\big) log(1-p) + Nlog(\\frac{p}{1-p}) \\big\\} \\\\\n",
        "&= \\underset{p}{argmax} \\big\\{\\overline{K} log(1-p) + log(\\frac{p}{1-p}) \\big\\} \\\\\n",
        "\\end{align}\n",
        "\n",
        "* Now we derive with respect to $p$ and find zero derivatives:\n",
        "\\begin{align}\n",
        "0 &\\stackrel{!}{=} \\frac{dl(p)}{dp} = \\frac{-\\overline{K}}{1-p} + \\frac{1}{p} - \\frac{-1}{1-p} \\\\\n",
        "\\frac{\\overline{K}}{1-p} &\\stackrel{!}{=} \\frac{(1-p) + p}{p(1-p)} \\\\\n",
        "\\overline{K} &\\stackrel{!}{=} \\frac{1}{p} \\\\\n",
        "p &\\stackrel{!}{=} \\frac{1}{\\overline{K}}\n",
        "\\end{align}\n",
        "    * This has to be a maximum, because $p=0$ and $p=1$ both minimize the likelyhood to be zero (assuming not all $K_i$ equal only $0$s or only $\\infty$s)\n",
        "    * This makes sense because if $ K \\sim Geom(p) $ then $ \\overline{K} = \\widehat{\\mathbb{E}[K]} = \\widehat{\\frac{1}{p}} \\Longrightarrow \\hat{p} = \\frac{1}{\\overline{K}} $\n",
        "    * To conclude, we got $ \\hat{p}_{MLE} = \\frac{1}{\\overline{K}} $\n",
        "\n",
        "\n",
        "5. We calculate the probability for odd $K$ by using a sum that covers all $k$s (not just the odd ones) but **zeros any even $k$**:\n",
        "\\begin{align}\n",
        "P(K \\text{ is odd}) = \\sum_{k \\in \\{1,3,..\\}} P(K = k)\n",
        "&= \\sum_{k=1}^\\infty \\frac{1 + (-1)^{k-1}}{2} P(K = k) \\\\\n",
        "&= \\sum_{k=1}^\\infty \\frac{1 + (-1)^{k-1}}{2} (1-p)^{k-1}p \\\\\n",
        "&= \\frac{1}{2}\\Big(\\sum_{k=1}^\\infty \\big((1-p)^{k-1}p\\big) + \\sum_{k=1}^\\infty \\big((-1)^{k-1}(1-p)^{k-1}p\\big) \\Big) \\\\\n",
        "&= \\frac{1}{2}\\Big(1 + p\\sum_{k=1}^\\infty (p-1)^{k-1} \\Big) \\\\\n",
        "&= \\frac{1}{2}\\Big(1 + p\\sum_{k=0}^\\infty (p-1)^{k} \\Big) \\\\\n",
        "&= \\frac{1}{2}\\Big(1 + p \\cdot \\frac{1}{1-(p-1)} \\Big) \\\\\n",
        "&= \\frac{1}{2}\\Big(1 + \\frac{p}{2-p} \\Big) \\\\\n",
        "&= \\frac{1}{2-p}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Ic6DE12cKS",
        "outputId": "e4a9cf6f-7f75-4c34-f46e-b8d347587ebd"
      },
      "source": [
        "\n",
        "# imports\n",
        "import numpy as np\n",
        "\n",
        "# 3.\n",
        "N = 100\n",
        "alpha = np.array([0.05])\n",
        "epsilon = np.sqrt(np.log(2/alpha) / (2*N))\n",
        "print(f\"Epsilons corresponding to alphas {alpha} are {epsilon}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epsilons corresponding to alphas [0.05] are [0.13581015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHllIu_4d070"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Question 4 - Python - Parkinson's Disease Classification Data Analysis\n",
        "---\n",
        "In this exercise, we are going to do data analysis with Python and Pandas. As this is the first \"real\" exercise, we will add guidance for some of the tasks.\n",
        "\n",
        "1. Warmup - Generate 100 samples from $\\mathcal{N}(0,1)$ (`np.random.randn`). Compute a 95% CI for the CDF. Plot the true CDF, the CDF estimation and the CI in a single plot. To estimate $\\hat{F}_n$ use a histogram (`np.histogram`). Repeat this $K=1000$ times and compute the percentage of time that the interval contained the CDF (print the value) . In addition, plot in another single figure the *true* CDF, and the best and worst experiments (use $\\max_x|F(x) - \\hat{F}_n(x)|$ as quality measure).\n",
        "    * To compare np arrays element-wise use `np.less_equal(x1, x2 + eps), np.greater_equal(x1, x2 - eps)`, use `.all()` to verify if all the comaprisons were `True`.\n",
        "    \n",
        "We are now going to perform some real data analysis on the \"Parkinson's Disease Classification Data Set\": the data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 at the Department of Neurology in CerrahpaÅŸa Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82. During the data collection process, the microphone is set to 44.1 KHz and following the examination, the sustained phonation of the vowel /a/ was collected from each subject with three repetitions. \n",
        "\n",
        "The features are various speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n",
        "\n",
        "2. Load the data with pandas, drop the 'id' column, take a sample ($k=10$, `dataframe.sample(k)`) and view it.\n",
        "    * The filename is `pd_speech_features.csv`.\n",
        "3. Compute the empirical correlation between all pairs of features. Show the results both in a heatmap.\n",
        "    * Use pandas `.corr()` to calculate the correaltion, and `plt.imshow()` to view the heatmap (2 heatmaps, one for the correlation and one for the absolute correlation). Add a color bar using `plt.colorbar()`\n",
        "4. Print the top-20 most correlated features. Follow this steps:\n",
        "    * Take the lower triangle of the correlation matrix (as it is symmetrical and we don't care about $Corr(X_i,X_i)$). Use `np.tril()`\n",
        "    * Consider only positive correlation (because negative correlation has a different, useful meaning). You can do that by `X = X[X >0]`.\n",
        "    * From here, these are recommended steps, feel free to achieve the goal in a different way.\n",
        "        * Assignment to a pandas DataFrame: `X.loc[:,:] = np.(...)`\n",
        "        * Unstacking the DataFrame (creates a new pivot, read the doc): `df.unstack()`\n",
        "        * Sorting: `df.sort()`\n",
        "\n",
        "5. What is the meaning when 2 different features are highly correlated? From a machine learning perspective, can a classifier learn new insights from highly-correlated features? In your answer, address the process of \"feature selection\" in ML (usually performed as a pre-processing step).\n",
        "\n",
        "6. Compute the **in-class** correlation between features. Plot a heat map for each class. Address the differences between the heat maps.\n",
        "\n",
        "7. Consider the features 'numPulses' and 'app_entropy_log_5_coef'. We wish to calculate a 95% confidence interval for the correlation between these features. We will use *Bootstrapping* and the *Chebyshev inequality* (as in Tutorial 2).\n",
        "    * Implement the bootstrap algorithm to calculate the standard deviation ($\\sigma$) of the correlation.\n",
        "        * You can use the algorithm from the tutorial, but you have to modify it to support 2 arrays.\n",
        "        * The algorithm will output the empirical correlation of the two input features, and a bootstrap estimation for the std ($\\sigma$). Use `K=300` bootstrap samples and `m=100` experiments.\n",
        "        * Tips:\n",
        "            * To get values for two columns: `data[['numPulses', 'app_entropy_log_5_coef']]`\n",
        "            * To calculate correlation, check out `numpy.corrcoef`.\n",
        "    * Use $\\sigma$ to calculate a 95% CI using Chebyshev inequality.\n",
        "        * Remember to to normalize by the size of the data ($N$).\n",
        "    The final print should look something like that: `95% confidence interval for the correlation: *** ± ***`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZRayQYbd070"
      },
      "source": [
        "# imports for q-4\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats  import norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo74uT4Dd074"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ]
}